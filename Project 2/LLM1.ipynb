{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f53ed2-e127-479b-b288-6ba1e291eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a43d108-7f2d-4d45-bc22-c4843c5a0c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6847d575-f208-4c43-8d20-0314f1acd9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f025b348-7734-4690-88f3-0790bbfb7128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyDjL6WhNL553hM6HJetasm8_1tT-qwJBWg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b743cd7-b764-4139-98d5-525ca1b0506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71e7ee3f-0862-4ef9-974f-2fcdde419b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Hello, Gemini!! This my first message to you! Hi!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f431c7f-2cf0-455c-8c0d-70beecfe1637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id=None, choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! It's wonderful to hear from you! I'm excited to receive your first message. Hi back! ðŸ‘‹  How are you doing today? Is there anything I can help you with, or are you just saying hello?  Either way, I'm happy to chat.\\n\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1740232287, model='gemini-2.0-pro-exp-02-05', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=59, prompt_tokens=14, total_tokens=73, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gemini-2.0-pro-exp-02-05\", messages=[{\"role\":'user', \"content\":message}])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9cf50ee-d710-4b66-863f-9e5df089c0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's wonderful to hear from you! I'm excited to receive your first message. Hi back! ðŸ‘‹  How are you doing today? Is there anything I can help you with, or are you just saying hello?  Either way, I'm happy to chat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1cf18e-024e-4e88-be5a-a9a98b99b49a",
   "metadata": {},
   "source": [
    "## Web Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3da7815-27bc-4f70-a233-208727b9a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24988963-a97b-44c2-a856-8c3be13fdb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        self.title = soup.title.string if soup.title else \"No Title found\"\n",
    "\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01fda569-ea0c-4086-96a4-193eee5417e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "web = Website(\"https://www.geeksforgeeks.org/exploring-the-technical-architecture-behind-large-language-models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19a03a41-0573-4305-b9a1-f57cbe89d0a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Architecture: Exploring the Technical Architecture Behind Large Language Models - GeeksforGeeks\n",
      "Skip to content\n",
      "Courses\n",
      "Get IBM Certifications\n",
      "Complete Machine Learning & Data Science Program\n",
      "Data Science Training Program\n",
      "Data Analytics Training using Excel, SQL, Python & PowerBI\n",
      "Complete Data Analytics Program\n",
      "DSA to Development\n",
      "For Working Professionals\n",
      "Data Structure & Algorithm Classes (Live)\n",
      "System Design (Live)\n",
      "JAVA Backend Development(Live)\n",
      "DevOps(Live)\n",
      "Data Structures & Algorithms in Python\n",
      "For Students\n",
      "Interview Preparation Course\n",
      "GATE CS & IT 2025\n",
      "Data Science (Live)\n",
      "Data Structure & Algorithm-Self Paced(C++/JAVA)\n",
      "Master Competitive Programming(Live)\n",
      "Full Stack Development with React & Node JS(Live)\n",
      "All Courses\n",
      "Full Stack Development\n",
      "Data Science Program\n",
      "Tutorials\n",
      "Python\n",
      "Python Tutorial\n",
      "Python Programs\n",
      "Advanced Python Tutorial\n",
      "Python Projects\n",
      "Web Development in Python\n",
      "Django\n",
      "Flask\n",
      "Postman\n",
      "Web Scrapping in Python\n",
      "Data Structures & Algorithms\n",
      "Complete DSA Tutorial\n",
      "Company Wise Preparation\n",
      "Competitive Programming\n",
      "SDE Sheets\n",
      "Languages\n",
      "Java\n",
      "C++\n",
      "R Tutorial\n",
      "C\n",
      "C#\n",
      "SQL\n",
      "Perl\n",
      "Go Language\n",
      "Interview Corner\n",
      "System Design Tutorial\n",
      "Top Topics\n",
      "Practice Company Questions\n",
      "Interview Experiences\n",
      "Experienced Interviews\n",
      "Internship Interviews\n",
      "Multiple Choice Quizzes\n",
      "Aptitude for Placements\n",
      "Puzzles for Interviews\n",
      "Web Development\n",
      "HTML\n",
      "CSS\n",
      "JavaScript\n",
      "TypeScript\n",
      "ReactJS\n",
      "NextJS\n",
      "Node.js\n",
      "PHP\n",
      "100 Days of Web Development\n",
      "CS Subjects\n",
      "Engineering Mathematics\n",
      "Operating System\n",
      "DBMS\n",
      "Computer Networks\n",
      "Computer Organization and Architecture\n",
      "Theory of Computation\n",
      "Compiler Design\n",
      "Digital Logic\n",
      "Software Engineering\n",
      "DevOps And Linux\n",
      "DevOps Tutorial\n",
      "GIT\n",
      "AWS\n",
      "Kubernetes\n",
      "Docker\n",
      "Microsoft Azure Tutorial\n",
      "Google Cloud Platform\n",
      "Linux Tutorial\n",
      "GATE\n",
      "GATE Computer Science Notes\n",
      "GATE CS Original Papers and Official Keys\n",
      "GATE CS 2025 Syllabus\n",
      "GATE DA 2025 Syllabus\n",
      "School Learning\n",
      "GeeksforGeeks Videos\n",
      "Data Science\n",
      "Machine Learning\n",
      "EDA [Exploratory Data Analysis]\n",
      "Data Analysis with Python\n",
      "Python Data Visualization\n",
      "Data Science using Python\n",
      "Data Science using R\n",
      "Deep Learning\n",
      "NLP Tutorial\n",
      "Computer Vision\n",
      "Interview Corner\n",
      "Machine Learning Interview Question\n",
      "Deep Learning Interview Question\n",
      "NLP Interview Question\n",
      "Python Interview Questions\n",
      "Top 50 R Interview Questions\n",
      "Practice\n",
      "All DSA Problems\n",
      "Problem of the Day\n",
      "GfG SDE Sheet\n",
      "Curated DSA Lists\n",
      "Beginner's DSA Sheet\n",
      "Love Babbar Sheet\n",
      "Top 50 Array Problems\n",
      "Top 50 String Problems\n",
      "Top 50 Tree Problems\n",
      "Top 50 Graph Problems\n",
      "Top 50 DP Problems\n",
      "Notifications\n",
      "Mark all as read\n",
      "All\n",
      "View All\n",
      "Notifications\n",
      "Mark all as read\n",
      "All\n",
      "Unread\n",
      "Read\n",
      "You're all caught up!!\n",
      "Data Science IBM Certification\n",
      "Data Science\n",
      "Data Science Projects\n",
      "Data Analysis\n",
      "Data Visualization\n",
      "Machine Learning\n",
      "ML Projects\n",
      "Deep Learning\n",
      "NLP\n",
      "Computer Vision\n",
      "Artificial Intelligence\n",
      "â–²\n",
      "Open In App\n",
      "GfG 160\n",
      "Share Your Experiences\n",
      "LLM Architecture: Exploring the Technical Architecture Behind Large Language Models\n",
      "Exploring Multimodal Large Language Models\n",
      "LLMs vs. SLMs : Comparative Analysis of Language Model Architectures\n",
      "What is a Large Language Model (LLM)\n",
      "LLM vs GPT : Comparing Large Language Models and GPT\n",
      "10 Exciting Project Ideas Using Large Language Models (LLMs)\n",
      "Discounting Techniques in Language Models\n",
      "Fine Tuning Large Language Model (LLM)\n",
      "Multiturn Deviation in Large Language Model\n",
      "10 Free Resources to Learn Large Language Models (LLMs)\n",
      "Enhancing Natural Language Processing with Transfer Learning: Techniques, Models, and Applications\n",
      "Gemma vs. Gemini vs. LLM (Large Language Model)\n",
      "Future of Large Language Models\n",
      "Ollama Explained: Transforming AI Accessibility and Language Processing\n",
      "Exploring Llama-3-8b-Instruct: Advancements, Applications, and Future Prospects in AI Language Models\n",
      "From GPT-3 to GPT-4: Evolution and Innovations in Large Language Models\n",
      "What is PaLM 2: Google's Large Language Model Explained\n",
      "Advanced Smoothing Techniques in Language Models\n",
      "Top 20 LLM (Large Language Models)\n",
      "ELIZA: The First Step in Human-Computer Interaction Through Natural Language Processing\n",
      "DSA to Development\n",
      "Course\n",
      "LLM Architecture: Exploring the Technical Architecture Behind Large Language Models\n",
      "Last Updated :\n",
      "20 Sep, 2024\n",
      "Summarize\n",
      "Comments\n",
      "Improve\n",
      "Suggest changes\n",
      "Like Article\n",
      "Like\n",
      "Share\n",
      "Report\n",
      "Follow\n",
      "Large Language Models (LLMs)\n",
      "have become a cornerstone in the field of artificial intelligence, driving advancements in natural language processing (NLP), conversational AI, and various applications that require understanding and generating human-like text. The technical architecture of these models is a complex interplay of several components, each designed to maximize performance, scalability, and accuracy.\n",
      "Exploring the Technical Architecture Behind Large Language Models\n",
      "In this article, we will delve into the\n",
      "key aspects of the technical architecture of LLMs, exploring their structure, training processes, and the innovations that power them.\n",
      "Table of Content\n",
      "Introduction to Large Language Model Architecture\n",
      "Architecture of Large Language Models (LLMs)\n",
      "Input Layer: Tokenization\n",
      "Embedding Layer\n",
      "Transformer Architecture\n",
      "Stacking Layers\n",
      "Output Layer: Decoding\n",
      "Optimization\n",
      "Scaling and Parallelism\n",
      "Ethical Considerations\n",
      "Introduction to Large Language Model Architecture\n",
      "Large Language Models\n",
      "are designed to understand and generate human language. Modern language models, particularly those built on transformer architectures, have revolutionized the field with their ability to process and generate text with high accuracy and relevance. The technical architecture of these models is both complex and fascinating, involving several key components and mechanisms.\n",
      "Architecture of Large Language Models (LLMs)\n",
      "Architecture of Large Language Models (LLMs)\n",
      "Large Language Models (LLMs)\n",
      "like GPT-4, BERT, and others are complex systems designed to process and generate human-like text. Their architecture involves multiple layers and components, each contributing to the model's ability to understand and produce language. Here's an overview of the\n",
      "key components and the architecture of LLMs\n",
      ":\n",
      "Input Layer: Tokenization\n",
      "Tokenization:\n",
      "The input text is broken down into smaller units called tokens, which can be words, subwords, or characters. These tokens are then converted into numerical representations (embeddings) that the model can process.\n",
      "Embedding Layer\n",
      "Word Embeddings:\n",
      "Each token is mapped to a dense vector in a high-dimensional space, representing its semantic meaning. Common techniques include Word2Vec, GloVe, and embeddings learned during model training.\n",
      "Positional Embeddings:\n",
      "Since transformers do not inherently understand the order of tokens, positional embeddings are added to the word embeddings to give the model information about the token positions within a sentence.\n",
      "Transformer Architecture\n",
      "Self-Attention Mechanism:\n",
      "Attention Scores:\n",
      "The self-attention mechanism computes a set of attention scores that determine how much focus each word should give to other words in the sequence.\n",
      "Query, Key, and Value (Q, K, V):\n",
      "These are linear projections of the input embeddings used to compute attention. The model calculates the relevance of each token to others using the dot product of Query and Key vectors, followed by a softmax operation to obtain attention weights. The Value vectors are then weighted by these attention scores.\n",
      "Multi-Head Attention:\n",
      "Multiple attention heads are used to capture different aspects of the relationships between tokens. Each head operates in a separate subspace, and the results are concatenated and projected back into the original space.\n",
      "Feedforward\n",
      "Neural Network\n",
      ":\n",
      "After the attention mechanism, the output is passed through a feedforward neural network (a series of dense layers with activation functions), applied independently to each position.\n",
      "Layer Normalization and Residual Connections:\n",
      "Each sub-layer (attention and feedforward) is followed by layer normalization and a residual connection, which helps stabilize training and allows for deeper networks.\n",
      "Stacking Layers\n",
      "Transformer Blocks:\n",
      "The architecture typically involves stacking multiple transformer layers (or blocks) on top of each other. Each block consists of a multi-head self-attention mechanism and a feedforward neural network. This stacking allows the model to learn complex hierarchical representations of the data.\n",
      "Output Layer: Decoding\n",
      "Language Modeling Objective:\n",
      "In autoregressive models like GPT, the model is trained to predict the next token in a sequence given the previous tokens. In masked language models like BERT, the model predicts missing tokens in a sequence.\n",
      "Softmax Layer:\n",
      "The final layer is typically a softmax function that converts the model's output into a probability distribution over the vocabulary, allowing it to select the most likely next token or fill in a masked token.\n",
      "Training and Fine-Tuning\n",
      "Pre-Training:\n",
      "Data Collection:\n",
      "Large-scale language models are pre-trained on diverse and extensive datasets that include books, articles, websites, and other text sources. This helps the model learn a broad understanding of language and context.\n",
      "Objective Functions:\n",
      "During pre-training, models typically use objective functions such as masked language modeling (MLM) or autoregressive modeling. MLM involves predicting missing words in a sentence, while autoregressive modeling focuses on predicting the next word in a sequence.\n",
      "Computational Resources:\n",
      "Training these models requires powerful hardware, including GPUs or TPUs, and substantial memory. Distributed computing and parallel processing are often employed to handle the computational demands.\n",
      "Fine-Tuning:\n",
      "Domain-Specific Training:\n",
      "After pre-training, models are fine-tuned on specific tasks or domains. Fine-tuning involves additional training on more specialized datasets to adapt the model to particular applications, such as sentiment analysis or machine translation.\n",
      "Hyperparameter Tuning\n",
      ":\n",
      "Fine-tuning also involves adjusting hyperparameters, such as learning rates and batch sizes, to optimize the model's performance for the target task.\n",
      "Optimization\n",
      "Loss Function\n",
      ":\n",
      "The model is trained to minimize a loss function, typically cross-entropy loss, which measures the difference between the predicted probability distribution and the actual distribution.\n",
      "Gradient Descent and Backpropagation:\n",
      "The model's parameters are updated using gradient descent and backpropagation to minimize the loss function across many iterations.\n",
      "Scaling and Parallelism\n",
      "Model Scaling:\n",
      "Modern LLMs often contain billions of parameters, requiring significant computational resources. Techniques like model parallelism, data parallelism, and distributed training are used to handle the computational load.\n",
      "Inference Optimization:\n",
      "To make LLMs efficient at inference time, techniques like quantization, pruning, and distillation are often used to reduce model size and improve speed without significantly compromising performance.\n",
      "Ethical Considerations\n",
      "Bias and Fairness:\n",
      "LLMs can inherit biases present in the training data, leading to problematic outputs. Addressing these issues involves careful dataset curation, bias mitigation techniques, and ongoing monitoring.\n",
      "Safety and Robustness:\n",
      "Ensuring that LLMs produce safe and reliable outputs, especially in sensitive applications, is a critical concern. This involves implementing safeguards against harmful content and adversarial attacks.\n",
      "Conclusion\n",
      "The technical architecture behind modern language models is a marvel of engineering and innovation. The transformer architecture, with its self-attention mechanisms, positional encoding, and multi-head attention, has set the foundation for the remarkable capabilities of these models. Advances in training techniques, fine-tuning strategies, and architectural innovations continue to drive the evolution of language models, making them more powerful, efficient, and versatile.\n",
      "As language models become increasingly sophisticated, understanding their architecture helps us appreciate the complexity and potential of these technologies. Whether you're a researcher, developer, or enthusiast, exploring the technical details of language models offers valuable insights into how these AI systems are shaping the future of human-computer interaction and natural language understanding.\n",
      "Get IBM Certification\n",
      "and a\n",
      "90% fee refund\n",
      "on completing 90% course in 90 days!\n",
      "Take the Three 90 Challenge today.\n",
      "Master Machine Learning, Data Science & AI with this complete program and also get a 90% refund. What more motivation do you need?\n",
      "Start the challenge right away!\n",
      "Comment\n",
      "More info\n",
      "Advertise with us\n",
      "Next Article\n",
      "Exploring Multimodal Large Language Models\n",
      "P\n",
      "poonamvbo5\n",
      "Follow\n",
      "Improve\n",
      "Article Tags :\n",
      "AI-ML-DS Blogs\n",
      "AI-ML-DS\n",
      "AI Blogs\n",
      "Similar Reads\n",
      "Top 20 LLM (Large Language Models)\n",
      "Large Language Model commonly known as an LLM, refers to a neural network equipped with billions of parameters and trained extensively on extensive datasets of unlabeled text. This training typically involves self-supervised or semi-supervised learning techniques. In this article, we explore about Top 20 LLM Models and get to know how each model ha\n",
      "15+ min read\n",
      "LLM vs GPT : Comparing Large Language Models and GPT\n",
      "In recent years, the field of natural language processing (NLP) has made tremendous strides, largely due to the development of large language models (LLMs) and, more specifically, the Generative Pre-trained Transformer (GPT) series. Both LLMs and GPTs have transformed how machines understand and generate human language. Table of Content What is a L\n",
      "4 min read\n",
      "Exploring Multimodal Large Language Models\n",
      "Multimodal large language models (LLMs) integrate and process diverse types of data (such as text, images, audio, and video) to enhance understanding and generate comprehensive responses. The article aims to explore the evolution, components, importance, and examples of multimodal large language models (LLMs) integrating text, images, audio, and vi\n",
      "8 min read\n",
      "Gemma vs. Gemini vs. LLM (Large Language Model)\n",
      "Artificial Intelligence (AI) has witnessed exponential growth, with language models at the forefront of many transformative applications. Three key players in this space, Gemma, Gemini, and LLMs (Large Language Models), represent cutting-edge advancements in AI-driven conversational agents and data processing. While they share common goals, these t\n",
      "5 min read\n",
      "Fine Tuning Large Language Model (LLM)\n",
      "Large Language Models (LLMs) have dramatically transformed natural language processing (NLP), excelling in tasks like text generation, translation, summarization, and question-answering. However, these models may not always be ideal for specific domains or tasks. To address this, fine-tuning is performed. Fine-tuning customizes pre-trained LLMs to\n",
      "13 min read\n",
      "What is a Large Language Model (LLM)\n",
      "Large Language Models (LLMs) represent a breakthrough in artificial intelligence, employing neural network techniques with extensive parameters for advanced language processing. This article explores the evolution, architecture, applications, and challenges of LLMs, focusing on their impact in the field of Natural Language Processing (NLP). What ar\n",
      "10 min read\n",
      "Exploring Llama-3-8b-Instruct: Advancements, Applications, and Future Prospects in AI Language Models\n",
      "In the rapidly evolving landscape of artificial intelligence, language models play a crucial role in driving innovation and transforming various industries. One of the latest advancements in this domain is the Llama-3-8b-Instruct model, a powerful AI tool designed to enhance natural language understanding and generation. This article delves into th\n",
      "7 min read\n",
      "Build RAG pipeline using Open Source Large Language Models\n",
      "In this article, we will implement Retrieval Augmented Generation aka RAG pipeline using Open-Source Large Language models with Langchain and HuggingFace. Open Source LLMs vs Closed Source LLMsLarge Language models are all over the place. Because of the rise of Large Language models, AI came into the limelight in the market. From development to the\n",
      "8 min read\n",
      "Future of Large Language Models\n",
      "In the last few years, the development of artificial intelligence has been in significant demand, with the emergence of Large Language Models (LLMs). This streamlined model entails advanced machine learning methods, has transformed natural language procedures, and is expected to revolutionize the future of human-tech or computer interaction seamles\n",
      "8 min read\n",
      "Large Language Models (LLMs) vs Transformers\n",
      "In recent years, advancements in artificial intelligence have led to the development of sophisticated models that are capable of understanding and generating human-like text. Two of the most significant innovations in this space are Large Language Models (LLMs) and Transformers. While they are often discussed together, they serve different purposes\n",
      "7 min read\n",
      "Top  20  Applications of Large Language Models in Real-Life\n",
      "Language models (LLMs), such as GPT-4, have revolutionized numerous industries by leveraging their advanced capabilities in natural language processing (NLP) to enhance efficiency, accuracy, and user experience. From automating tasks to providing personalized services, these models have become indispensable tools across various sectors. This articl\n",
      "8 min read\n",
      "From GPT-3 to GPT-4: Evolution and Innovations in Large Language Models\n",
      "The progression of artificial intelligence (AI) in recent years has been nothing short of extraordinary, with significant strides particularly evident in the realm of natural language processing (NLP). Central to this advancement is the development of large language models (LLMs) like OpenAI's GPT series. This article explores the evolution from GP\n",
      "7 min read\n",
      "10 Free Resources to Learn Large Language Models (LLMs)\n",
      "Large Language Models (LLMs), such as GPT-3 and its successors, are reshaping how we interact with technology. From generating text to answering questions, LLMs are becoming an integral part of many applications. If you're eager to dive into the world of LLMs but don't know where to start, this guide provides 10 Free Resources to Learn Large Langua\n",
      "6 min read\n",
      "Exploring Generative Models: Applications, Examples, and Key Concepts\n",
      "A generative model is a type of machine learning model that aims to learn underlying patterns or distributions of data to generate new, similar data. This is used in unsupervised machine learning to describe phenomena in data, enabling computers to understand the real world. In this article, we will discuss some applications and examples of generat\n",
      "9 min read\n",
      "Rabbit AI: Large Action Models (LAMs)\n",
      "The Large Action Models (LAMs) are advanced artificial intelligence systems that are capable of understanding the human intention and predicting actions. In this article, we will be covering the fundamentals, working and architecture of the Large Action Models. We have all heard about Generative AI & LLMs, used them, and seen their tremendous i\n",
      "9 min read\n",
      "Microsoftâ€™s Large Action Models (LAM): Features, Applications, and Future\n",
      "Microsoft has unveiled its latest innovation in artificial intelligence, Large Action Models (LAM), which go beyond traditional language models to tackle complex tasks with a focus on automation and optimization. Unlike Large Language Models, which prioritize understanding and generating human-like text, LAM is designed for actionable decision-maki\n",
      "9 min read\n",
      "Develop an LLM Application using Openai\n",
      "Language Models (LMs) play a crucial role in natural language processing applications, enabling the development of tools that generate human-like text. OpenAI's Generative Pre-Trained Transformer (GPT) models, such as GPT-3.5-turbo, are widely used in this domain. They excel in understanding context, learning language patterns, and generating coher\n",
      "5 min read\n",
      "Can LLM replace Data Analyst\n",
      "As we know, today's era is all about data, as the quantity of data is increasing daily. Data analysis is the process of extracting, cleaning, and preprocessing the data and gathering insights from the data. Nowadays, there is also a trend of large language models such as ChatGPT4, so many business analysts use large language models to solve their p\n",
      "7 min read\n",
      "Falcon LLM: Comprehensive Guide\n",
      "Falcon LLM is a large language model that is engineered to comprehend and generate human like text, showcasing remarkable improvements in natural language and generation capabilities. This article covers the fundamentals of Falcon LLM and demonstrates how can we perform text generation using Falcon LLM. Table of Content What is Falcon LLM? Key Feat\n",
      "11 min read\n",
      "Pi: World's Friendliest Chatbot Gets Even Smarter with Inflection-2.5 LLM\n",
      "The world of conversational AI, powered by large language models (LLMs), is rapidly evolving. Inflection AI, a leader in this field, is pushing the boundaries with its innovative chatbot, Pi. Known for its friendly personality, Pi is about to get even smarter thanks to a groundbreaking upgrade â€“ the Inflection-2.5 LLM. This cutting-edge AI technolo\n",
      "5 min read\n",
      "RAG Vs Fine-Tuning for Enhancing LLM Performance\n",
      "Data Science and Machine Learning researchers and practitioners alike are constantly exploring innovative strategies to enhance the capabilities of language models. Among the myriad approaches, two prominent techniques have emerged which are Retrieval-Augmented Generation (RAG) and Fine-tuning. The article aims to explore the importance of model pe\n",
      "9 min read\n",
      "NLP vs LLM: Understanding Key Differences\n",
      "In the rapidly evolving field of artificial intelligence, two concepts that often come into focus are Natural Language Processing (NLP) and Large Language Models (LLM). Although they are intertwined, each plays a distinct role in how machines understand and generate human language. This article delves into the definitions, differences, and intercon\n",
      "6 min read\n",
      "Securing LLM Systems Against Prompt Injection\n",
      "Large Language Models (LLMs) have revolutionized the field of artificial intelligence, enabling applications such as chatbots, content generators, and personal assistants. However, the integration of LLMs into various applications has introduced new security vulnerabilities, notably prompt injection attacks. These attacks exploit the way LLMs proce\n",
      "11 min read\n",
      "Prompt Injection in LLM\n",
      "Prompt injection is a significant and emerging concern in the field of artificial intelligence, particularly in the context of large language models (LLMs). As these models become increasingly sophisticated and integrated into various applications, understanding and mitigating prompt injection becomes essential. This article delves into the intrica\n",
      "8 min read\n",
      "Open Sorce LLM : Hugging Face Impacts on the AI Community\n",
      "Hugging Face, a leading force in natural language processing (NLP), has made a profound impact on the AI community through its commitment to open-source principles. By offering advanced NLP models and tools for free, Hugging Face has reshaped the landscape of AI research and development. This article explores how Hugging Faceâ€™s open-source approach\n",
      "4 min read\n",
      "How to Fine-Tune an LLM from Hugging Face\n",
      "Large Language Models (LLMs) have transformed different tasks in natural language processing (NLP) such as translation, summarization, and text generation. Hugging Face's Transformers library offers a wide range of pre-trained models that can be customized for specific purposes through fine-tuning. Adjusting an LLM with task-specific data through f\n",
      "5 min read\n",
      "Explained LLM Leaderboard - 2024\n",
      "Artificial intelligence (AI) has expanded quickly, and there are great improvements in the use of large language models (LLMs). As these models advance, there is a shortage of a good system for evaluating and rating them appropriately. In this respect, the proposed LLM Leaderboard 2024 fulfils the criteria mentioned above by providing a set of stan\n",
      "10 min read\n",
      "Introduction to NExT-GPT: Any-to-Any Multimodal LLM\n",
      "The field of artificial intelligence (AI) has seen rapid advancements in recent years, particularly in the development of large language models (LLMs) like GPT-4. These models have primarily focused on text-based tasks, excelling in natural language understanding and generation. However, as multimodal applicationsâ€”those that involve various forms o\n",
      "6 min read\n",
      "What is JanitorLLM : The Free Version LLM of Janitor AI 2025\n",
      "In an era where Artificial Intelligence and Natural Language Processing (NLP) are transforming industries, JanitorLLM stands out as a prominent example of a state-of-the-art language model that is accessible for free. Developed by Janitor AI, JanitorLLM is designed to offer advanced text generation, comprehension, and interaction capabilities witho\n",
      "6 min read\n",
      "What is LLM Distillation?\n",
      "The size and computational demands of Large Language Models (LLMs) have made them impractical for many real-world applications, especially on edge devices or resource-constrained environments. This is where LLM Distillation comes into play. LLM Distillation is a specialized form of Knowledge Distillation (KD) aimed at compressing large-scale langua\n",
      "5 min read\n",
      "Like\n",
      "34k+ interested Geeks\n",
      "Mastering Generative AI and ChatGPT\n",
      "Explore\n",
      "1k+ interested Geeks\n",
      "Artificial Intelligence for Kids - Complete AI Course for Beginners\n",
      "Explore\n",
      "1k+ interested Geeks\n",
      "Complete TensorFlow Course\n",
      "Explore\n",
      "Corporate & Communications Address:\n",
      "A-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305)\n",
      "Registered Address:\n",
      "K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305\n",
      "Advertise with us\n",
      "Company\n",
      "About Us\n",
      "Legal\n",
      "Privacy Policy\n",
      "Careers\n",
      "In Media\n",
      "Contact Us\n",
      "GFG Corporate Solution\n",
      "Placement Training Program\n",
      "Explore\n",
      "Job-A-Thon Hiring Challenge\n",
      "Hack-A-Thon\n",
      "GfG Weekly Contest\n",
      "Offline Classes (Delhi/NCR)\n",
      "DSA in JAVA/C++\n",
      "Master System Design\n",
      "Master CP\n",
      "GeeksforGeeks Videos\n",
      "Geeks Community\n",
      "Languages\n",
      "Python\n",
      "Java\n",
      "C++\n",
      "PHP\n",
      "GoLang\n",
      "SQL\n",
      "R Language\n",
      "Android Tutorial\n",
      "DSA\n",
      "Data Structures\n",
      "Algorithms\n",
      "DSA for Beginners\n",
      "Basic DSA Problems\n",
      "DSA Roadmap\n",
      "DSA Interview Questions\n",
      "Competitive Programming\n",
      "Data Science & ML\n",
      "Data Science With Python\n",
      "Data Science For Beginner\n",
      "Machine Learning\n",
      "ML Maths\n",
      "Data Visualisation\n",
      "Pandas\n",
      "NumPy\n",
      "NLP\n",
      "Deep Learning\n",
      "Web Technologies\n",
      "HTML\n",
      "CSS\n",
      "JavaScript\n",
      "TypeScript\n",
      "ReactJS\n",
      "NextJS\n",
      "NodeJs\n",
      "Bootstrap\n",
      "Tailwind CSS\n",
      "Python Tutorial\n",
      "Python Programming Examples\n",
      "Django Tutorial\n",
      "Python Projects\n",
      "Python Tkinter\n",
      "Web Scraping\n",
      "OpenCV Tutorial\n",
      "Python Interview Question\n",
      "Computer Science\n",
      "GATE CS Notes\n",
      "Operating Systems\n",
      "Computer Network\n",
      "Database Management System\n",
      "Software Engineering\n",
      "Digital Logic Design\n",
      "Engineering Maths\n",
      "DevOps\n",
      "Git\n",
      "AWS\n",
      "Docker\n",
      "Kubernetes\n",
      "Azure\n",
      "GCP\n",
      "DevOps Roadmap\n",
      "System Design\n",
      "High Level Design\n",
      "Low Level Design\n",
      "UML Diagrams\n",
      "Interview Guide\n",
      "Design Patterns\n",
      "OOAD\n",
      "System Design Bootcamp\n",
      "Interview Questions\n",
      "School Subjects\n",
      "Mathematics\n",
      "Physics\n",
      "Chemistry\n",
      "Biology\n",
      "Social Science\n",
      "English Grammar\n",
      "Commerce\n",
      "Accountancy\n",
      "Business Studies\n",
      "Economics\n",
      "Management\n",
      "HR Management\n",
      "Finance\n",
      "Income Tax\n",
      "Databases\n",
      "SQL\n",
      "MYSQL\n",
      "PostgreSQL\n",
      "PL/SQL\n",
      "MongoDB\n",
      "Preparation Corner\n",
      "Company-Wise Recruitment Process\n",
      "Resume Templates\n",
      "Aptitude Preparation\n",
      "Puzzles\n",
      "Company-Wise Preparation\n",
      "Companies\n",
      "Colleges\n",
      "Competitive Exams\n",
      "JEE Advanced\n",
      "UGC NET\n",
      "UPSC\n",
      "SSC CGL\n",
      "SBI PO\n",
      "SBI Clerk\n",
      "IBPS PO\n",
      "IBPS Clerk\n",
      "More Tutorials\n",
      "Software Development\n",
      "Software Testing\n",
      "Product Management\n",
      "Project Management\n",
      "Linux\n",
      "Excel\n",
      "All Cheat Sheets\n",
      "Recent Articles\n",
      "Free Online Tools\n",
      "Typing Test\n",
      "Image Editor\n",
      "Code Formatters\n",
      "Code Converters\n",
      "Currency Converter\n",
      "Random Number Generator\n",
      "Random Password Generator\n",
      "Write & Earn\n",
      "Write an Article\n",
      "Improve an Article\n",
      "Pick Topics to Write\n",
      "Share your Experiences\n",
      "Internships\n",
      "DSA/\n",
      "Placements\n",
      "DSA - Self Paced Course\n",
      "DSA in JavaScript - Self Paced Course\n",
      "DSA in Python - Self Paced\n",
      "C Programming Course Online - Learn C with Data Structures\n",
      "Complete Interview Preparation\n",
      "Master Competitive Programming\n",
      "Core CS Subject for Interview Preparation\n",
      "Mastering System Design: LLD to HLD\n",
      "Tech Interview 101 - From DSA to System Design [LIVE]\n",
      "DSA to Development [HYBRID]\n",
      "Placement Preparation Crash Course [LIVE]\n",
      "Development/\n",
      "Testing\n",
      "JavaScript Full Course\n",
      "React JS Course\n",
      "React Native Course\n",
      "Django Web Development Course\n",
      "Complete Bootstrap Course\n",
      "Full Stack Development - [LIVE]\n",
      "JAVA Backend Development - [LIVE]\n",
      "Complete Software Testing Course [LIVE]\n",
      "Android Mastery with Kotlin [LIVE]\n",
      "Machine Learning/\n",
      "Data Science\n",
      "Complete Machine Learning & Data Science Program - [LIVE]\n",
      "Data Analytics Training using Excel, SQL, Python & PowerBI - [LIVE]\n",
      "Data Science Training Program - [LIVE]\n",
      "Mastering Generative AI and ChatGPT\n",
      "Data Science Course with IBM Certification\n",
      "Programming Languages\n",
      "C Programming with Data Structures\n",
      "C++ Programming Course\n",
      "Java Programming Course\n",
      "Python Full Course\n",
      "Clouds/\n",
      "Devops\n",
      "DevOps Engineering\n",
      "AWS Solutions Architect Certification\n",
      "Salesforce Certified Administrator Course\n",
      "GATE\n",
      "GATE CS & IT Test Series - 2025\n",
      "GATE DA Test Series 2025\n",
      "GATE CS & IT Course - 2025\n",
      "GATE DA Course 2025\n",
      "GATE Rank Predictor\n",
      "@GeeksforGeeks, Sanchhaya Education Private Limited\n",
      ",\n",
      "All rights reserved\n",
      "We use cookies to ensure you have the best browsing experience on our website. By using our site, you\n",
      "        acknowledge that you have read and understood our\n",
      "Cookie Policy\n",
      "&\n",
      "Privacy Policy\n",
      "Got It !\n",
      "Improvement\n",
      "Suggest changes\n",
      "Suggest Changes\n",
      "Help us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\n",
      "Create Improvement\n",
      "Enhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\n",
      "Suggest Changes\n",
      "min 4 words, max CharLimit:2000\n",
      "What kind of Experience do you want to share?\n",
      "Interview Experiences\n",
      "Admission Experiences\n",
      "Career Journeys\n",
      "Work Experiences\n",
      "Campus Experiences\n",
      "Competitive Exam Experiences\n"
     ]
    }
   ],
   "source": [
    "print(web.title)\n",
    "print(web.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d56e124-325f-4f1a-852b-c4ade74629b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f9347a9-6b91-42df-af1f-6fd7b5b75ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe content of this website are as follows; \\\n",
    "    please provide a short summary of this website in markdown.\\\n",
    "    If it included news or announcements, then summarize these too.\\n\\n\"\n",
    "\n",
    "    user_prompt += website.text\n",
    "\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a3ec497-ef21-4da8-990e-e7a5289f4b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are looking at a website titled LLM Architecture: Exploring the Technical Architecture Behind Large Language Models - GeeksforGeeks\n",
      "The content of this website are as follows;     please provide a short summary of this website in markdown.    If it included news or announcements, then summarize these too.\n",
      "\n",
      "Skip to content\n",
      "Courses\n",
      "Get IBM Certifications\n",
      "Complete Machine Learning & Data Science Program\n",
      "Data Science Training Program\n",
      "Data Analytics Training using Excel, SQL, Python & PowerBI\n",
      "Complete Data Analytics Program\n",
      "DSA to Development\n",
      "For Working Professionals\n",
      "Data Structure & Algorithm Classes (Live)\n",
      "System Design (Live)\n",
      "JAVA Backend Development(Live)\n",
      "DevOps(Live)\n",
      "Data Structures & Algorithms in Python\n",
      "For Students\n",
      "Interview Preparation Course\n",
      "GATE CS & IT 2025\n",
      "Data Science (Live)\n",
      "Data Structure & Algorithm-Self Paced(C++/JAVA)\n",
      "Master Competitive Programming(Live)\n",
      "Full Stack Development with React & Node JS(Live)\n",
      "All Courses\n",
      "Full Stack Development\n",
      "Data Science Program\n",
      "Tutorials\n",
      "Python\n",
      "Python Tutorial\n",
      "Python Programs\n",
      "Advanced Python Tutorial\n",
      "Python Projects\n",
      "Web Development in Python\n",
      "Django\n",
      "Flask\n",
      "Postman\n",
      "Web Scrapping in Python\n",
      "Data Structures & Algorithms\n",
      "Complete DSA Tutorial\n",
      "Company Wise Preparation\n",
      "Competitive Programming\n",
      "SDE Sheets\n",
      "Languages\n",
      "Java\n",
      "C++\n",
      "R Tutorial\n",
      "C\n",
      "C#\n",
      "SQL\n",
      "Perl\n",
      "Go Language\n",
      "Interview Corner\n",
      "System Design Tutorial\n",
      "Top Topics\n",
      "Practice Company Questions\n",
      "Interview Experiences\n",
      "Experienced Interviews\n",
      "Internship Interviews\n",
      "Multiple Choice Quizzes\n",
      "Aptitude for Placements\n",
      "Puzzles for Interviews\n",
      "Web Development\n",
      "HTML\n",
      "CSS\n",
      "JavaScript\n",
      "TypeScript\n",
      "ReactJS\n",
      "NextJS\n",
      "Node.js\n",
      "PHP\n",
      "100 Days of Web Development\n",
      "CS Subjects\n",
      "Engineering Mathematics\n",
      "Operating System\n",
      "DBMS\n",
      "Computer Networks\n",
      "Computer Organization and Architecture\n",
      "Theory of Computation\n",
      "Compiler Design\n",
      "Digital Logic\n",
      "Software Engineering\n",
      "DevOps And Linux\n",
      "DevOps Tutorial\n",
      "GIT\n",
      "AWS\n",
      "Kubernetes\n",
      "Docker\n",
      "Microsoft Azure Tutorial\n",
      "Google Cloud Platform\n",
      "Linux Tutorial\n",
      "GATE\n",
      "GATE Computer Science Notes\n",
      "GATE CS Original Papers and Official Keys\n",
      "GATE CS 2025 Syllabus\n",
      "GATE DA 2025 Syllabus\n",
      "School Learning\n",
      "GeeksforGeeks Videos\n",
      "Data Science\n",
      "Machine Learning\n",
      "EDA [Exploratory Data Analysis]\n",
      "Data Analysis with Python\n",
      "Python Data Visualization\n",
      "Data Science using Python\n",
      "Data Science using R\n",
      "Deep Learning\n",
      "NLP Tutorial\n",
      "Computer Vision\n",
      "Interview Corner\n",
      "Machine Learning Interview Question\n",
      "Deep Learning Interview Question\n",
      "NLP Interview Question\n",
      "Python Interview Questions\n",
      "Top 50 R Interview Questions\n",
      "Practice\n",
      "All DSA Problems\n",
      "Problem of the Day\n",
      "GfG SDE Sheet\n",
      "Curated DSA Lists\n",
      "Beginner's DSA Sheet\n",
      "Love Babbar Sheet\n",
      "Top 50 Array Problems\n",
      "Top 50 String Problems\n",
      "Top 50 Tree Problems\n",
      "Top 50 Graph Problems\n",
      "Top 50 DP Problems\n",
      "Notifications\n",
      "Mark all as read\n",
      "All\n",
      "View All\n",
      "Notifications\n",
      "Mark all as read\n",
      "All\n",
      "Unread\n",
      "Read\n",
      "You're all caught up!!\n",
      "Data Science IBM Certification\n",
      "Data Science\n",
      "Data Science Projects\n",
      "Data Analysis\n",
      "Data Visualization\n",
      "Machine Learning\n",
      "ML Projects\n",
      "Deep Learning\n",
      "NLP\n",
      "Computer Vision\n",
      "Artificial Intelligence\n",
      "â–²\n",
      "Open In App\n",
      "GfG 160\n",
      "Share Your Experiences\n",
      "LLM Architecture: Exploring the Technical Architecture Behind Large Language Models\n",
      "Exploring Multimodal Large Language Models\n",
      "LLMs vs. SLMs : Comparative Analysis of Language Model Architectures\n",
      "What is a Large Language Model (LLM)\n",
      "LLM vs GPT : Comparing Large Language Models and GPT\n",
      "10 Exciting Project Ideas Using Large Language Models (LLMs)\n",
      "Discounting Techniques in Language Models\n",
      "Fine Tuning Large Language Model (LLM)\n",
      "Multiturn Deviation in Large Language Model\n",
      "10 Free Resources to Learn Large Language Models (LLMs)\n",
      "Enhancing Natural Language Processing with Transfer Learning: Techniques, Models, and Applications\n",
      "Gemma vs. Gemini vs. LLM (Large Language Model)\n",
      "Future of Large Language Models\n",
      "Ollama Explained: Transforming AI Accessibility and Language Processing\n",
      "Exploring Llama-3-8b-Instruct: Advancements, Applications, and Future Prospects in AI Language Models\n",
      "From GPT-3 to GPT-4: Evolution and Innovations in Large Language Models\n",
      "What is PaLM 2: Google's Large Language Model Explained\n",
      "Advanced Smoothing Techniques in Language Models\n",
      "Top 20 LLM (Large Language Models)\n",
      "ELIZA: The First Step in Human-Computer Interaction Through Natural Language Processing\n",
      "DSA to Development\n",
      "Course\n",
      "LLM Architecture: Exploring the Technical Architecture Behind Large Language Models\n",
      "Last Updated :\n",
      "20 Sep, 2024\n",
      "Summarize\n",
      "Comments\n",
      "Improve\n",
      "Suggest changes\n",
      "Like Article\n",
      "Like\n",
      "Share\n",
      "Report\n",
      "Follow\n",
      "Large Language Models (LLMs)\n",
      "have become a cornerstone in the field of artificial intelligence, driving advancements in natural language processing (NLP), conversational AI, and various applications that require understanding and generating human-like text. The technical architecture of these models is a complex interplay of several components, each designed to maximize performance, scalability, and accuracy.\n",
      "Exploring the Technical Architecture Behind Large Language Models\n",
      "In this article, we will delve into the\n",
      "key aspects of the technical architecture of LLMs, exploring their structure, training processes, and the innovations that power them.\n",
      "Table of Content\n",
      "Introduction to Large Language Model Architecture\n",
      "Architecture of Large Language Models (LLMs)\n",
      "Input Layer: Tokenization\n",
      "Embedding Layer\n",
      "Transformer Architecture\n",
      "Stacking Layers\n",
      "Output Layer: Decoding\n",
      "Optimization\n",
      "Scaling and Parallelism\n",
      "Ethical Considerations\n",
      "Introduction to Large Language Model Architecture\n",
      "Large Language Models\n",
      "are designed to understand and generate human language. Modern language models, particularly those built on transformer architectures, have revolutionized the field with their ability to process and generate text with high accuracy and relevance. The technical architecture of these models is both complex and fascinating, involving several key components and mechanisms.\n",
      "Architecture of Large Language Models (LLMs)\n",
      "Architecture of Large Language Models (LLMs)\n",
      "Large Language Models (LLMs)\n",
      "like GPT-4, BERT, and others are complex systems designed to process and generate human-like text. Their architecture involves multiple layers and components, each contributing to the model's ability to understand and produce language. Here's an overview of the\n",
      "key components and the architecture of LLMs\n",
      ":\n",
      "Input Layer: Tokenization\n",
      "Tokenization:\n",
      "The input text is broken down into smaller units called tokens, which can be words, subwords, or characters. These tokens are then converted into numerical representations (embeddings) that the model can process.\n",
      "Embedding Layer\n",
      "Word Embeddings:\n",
      "Each token is mapped to a dense vector in a high-dimensional space, representing its semantic meaning. Common techniques include Word2Vec, GloVe, and embeddings learned during model training.\n",
      "Positional Embeddings:\n",
      "Since transformers do not inherently understand the order of tokens, positional embeddings are added to the word embeddings to give the model information about the token positions within a sentence.\n",
      "Transformer Architecture\n",
      "Self-Attention Mechanism:\n",
      "Attention Scores:\n",
      "The self-attention mechanism computes a set of attention scores that determine how much focus each word should give to other words in the sequence.\n",
      "Query, Key, and Value (Q, K, V):\n",
      "These are linear projections of the input embeddings used to compute attention. The model calculates the relevance of each token to others using the dot product of Query and Key vectors, followed by a softmax operation to obtain attention weights. The Value vectors are then weighted by these attention scores.\n",
      "Multi-Head Attention:\n",
      "Multiple attention heads are used to capture different aspects of the relationships between tokens. Each head operates in a separate subspace, and the results are concatenated and projected back into the original space.\n",
      "Feedforward\n",
      "Neural Network\n",
      ":\n",
      "After the attention mechanism, the output is passed through a feedforward neural network (a series of dense layers with activation functions), applied independently to each position.\n",
      "Layer Normalization and Residual Connections:\n",
      "Each sub-layer (attention and feedforward) is followed by layer normalization and a residual connection, which helps stabilize training and allows for deeper networks.\n",
      "Stacking Layers\n",
      "Transformer Blocks:\n",
      "The architecture typically involves stacking multiple transformer layers (or blocks) on top of each other. Each block consists of a multi-head self-attention mechanism and a feedforward neural network. This stacking allows the model to learn complex hierarchical representations of the data.\n",
      "Output Layer: Decoding\n",
      "Language Modeling Objective:\n",
      "In autoregressive models like GPT, the model is trained to predict the next token in a sequence given the previous tokens. In masked language models like BERT, the model predicts missing tokens in a sequence.\n",
      "Softmax Layer:\n",
      "The final layer is typically a softmax function that converts the model's output into a probability distribution over the vocabulary, allowing it to select the most likely next token or fill in a masked token.\n",
      "Training and Fine-Tuning\n",
      "Pre-Training:\n",
      "Data Collection:\n",
      "Large-scale language models are pre-trained on diverse and extensive datasets that include books, articles, websites, and other text sources. This helps the model learn a broad understanding of language and context.\n",
      "Objective Functions:\n",
      "During pre-training, models typically use objective functions such as masked language modeling (MLM) or autoregressive modeling. MLM involves predicting missing words in a sentence, while autoregressive modeling focuses on predicting the next word in a sequence.\n",
      "Computational Resources:\n",
      "Training these models requires powerful hardware, including GPUs or TPUs, and substantial memory. Distributed computing and parallel processing are often employed to handle the computational demands.\n",
      "Fine-Tuning:\n",
      "Domain-Specific Training:\n",
      "After pre-training, models are fine-tuned on specific tasks or domains. Fine-tuning involves additional training on more specialized datasets to adapt the model to particular applications, such as sentiment analysis or machine translation.\n",
      "Hyperparameter Tuning\n",
      ":\n",
      "Fine-tuning also involves adjusting hyperparameters, such as learning rates and batch sizes, to optimize the model's performance for the target task.\n",
      "Optimization\n",
      "Loss Function\n",
      ":\n",
      "The model is trained to minimize a loss function, typically cross-entropy loss, which measures the difference between the predicted probability distribution and the actual distribution.\n",
      "Gradient Descent and Backpropagation:\n",
      "The model's parameters are updated using gradient descent and backpropagation to minimize the loss function across many iterations.\n",
      "Scaling and Parallelism\n",
      "Model Scaling:\n",
      "Modern LLMs often contain billions of parameters, requiring significant computational resources. Techniques like model parallelism, data parallelism, and distributed training are used to handle the computational load.\n",
      "Inference Optimization:\n",
      "To make LLMs efficient at inference time, techniques like quantization, pruning, and distillation are often used to reduce model size and improve speed without significantly compromising performance.\n",
      "Ethical Considerations\n",
      "Bias and Fairness:\n",
      "LLMs can inherit biases present in the training data, leading to problematic outputs. Addressing these issues involves careful dataset curation, bias mitigation techniques, and ongoing monitoring.\n",
      "Safety and Robustness:\n",
      "Ensuring that LLMs produce safe and reliable outputs, especially in sensitive applications, is a critical concern. This involves implementing safeguards against harmful content and adversarial attacks.\n",
      "Conclusion\n",
      "The technical architecture behind modern language models is a marvel of engineering and innovation. The transformer architecture, with its self-attention mechanisms, positional encoding, and multi-head attention, has set the foundation for the remarkable capabilities of these models. Advances in training techniques, fine-tuning strategies, and architectural innovations continue to drive the evolution of language models, making them more powerful, efficient, and versatile.\n",
      "As language models become increasingly sophisticated, understanding their architecture helps us appreciate the complexity and potential of these technologies. Whether you're a researcher, developer, or enthusiast, exploring the technical details of language models offers valuable insights into how these AI systems are shaping the future of human-computer interaction and natural language understanding.\n",
      "Get IBM Certification\n",
      "and a\n",
      "90% fee refund\n",
      "on completing 90% course in 90 days!\n",
      "Take the Three 90 Challenge today.\n",
      "Master Machine Learning, Data Science & AI with this complete program and also get a 90% refund. What more motivation do you need?\n",
      "Start the challenge right away!\n",
      "Comment\n",
      "More info\n",
      "Advertise with us\n",
      "Next Article\n",
      "Exploring Multimodal Large Language Models\n",
      "P\n",
      "poonamvbo5\n",
      "Follow\n",
      "Improve\n",
      "Article Tags :\n",
      "AI-ML-DS Blogs\n",
      "AI-ML-DS\n",
      "AI Blogs\n",
      "Similar Reads\n",
      "Top 20 LLM (Large Language Models)\n",
      "Large Language Model commonly known as an LLM, refers to a neural network equipped with billions of parameters and trained extensively on extensive datasets of unlabeled text. This training typically involves self-supervised or semi-supervised learning techniques. In this article, we explore about Top 20 LLM Models and get to know how each model ha\n",
      "15+ min read\n",
      "LLM vs GPT : Comparing Large Language Models and GPT\n",
      "In recent years, the field of natural language processing (NLP) has made tremendous strides, largely due to the development of large language models (LLMs) and, more specifically, the Generative Pre-trained Transformer (GPT) series. Both LLMs and GPTs have transformed how machines understand and generate human language. Table of Content What is a L\n",
      "4 min read\n",
      "Exploring Multimodal Large Language Models\n",
      "Multimodal large language models (LLMs) integrate and process diverse types of data (such as text, images, audio, and video) to enhance understanding and generate comprehensive responses. The article aims to explore the evolution, components, importance, and examples of multimodal large language models (LLMs) integrating text, images, audio, and vi\n",
      "8 min read\n",
      "Gemma vs. Gemini vs. LLM (Large Language Model)\n",
      "Artificial Intelligence (AI) has witnessed exponential growth, with language models at the forefront of many transformative applications. Three key players in this space, Gemma, Gemini, and LLMs (Large Language Models), represent cutting-edge advancements in AI-driven conversational agents and data processing. While they share common goals, these t\n",
      "5 min read\n",
      "Fine Tuning Large Language Model (LLM)\n",
      "Large Language Models (LLMs) have dramatically transformed natural language processing (NLP), excelling in tasks like text generation, translation, summarization, and question-answering. However, these models may not always be ideal for specific domains or tasks. To address this, fine-tuning is performed. Fine-tuning customizes pre-trained LLMs to\n",
      "13 min read\n",
      "What is a Large Language Model (LLM)\n",
      "Large Language Models (LLMs) represent a breakthrough in artificial intelligence, employing neural network techniques with extensive parameters for advanced language processing. This article explores the evolution, architecture, applications, and challenges of LLMs, focusing on their impact in the field of Natural Language Processing (NLP). What ar\n",
      "10 min read\n",
      "Exploring Llama-3-8b-Instruct: Advancements, Applications, and Future Prospects in AI Language Models\n",
      "In the rapidly evolving landscape of artificial intelligence, language models play a crucial role in driving innovation and transforming various industries. One of the latest advancements in this domain is the Llama-3-8b-Instruct model, a powerful AI tool designed to enhance natural language understanding and generation. This article delves into th\n",
      "7 min read\n",
      "Build RAG pipeline using Open Source Large Language Models\n",
      "In this article, we will implement Retrieval Augmented Generation aka RAG pipeline using Open-Source Large Language models with Langchain and HuggingFace. Open Source LLMs vs Closed Source LLMsLarge Language models are all over the place. Because of the rise of Large Language models, AI came into the limelight in the market. From development to the\n",
      "8 min read\n",
      "Future of Large Language Models\n",
      "In the last few years, the development of artificial intelligence has been in significant demand, with the emergence of Large Language Models (LLMs). This streamlined model entails advanced machine learning methods, has transformed natural language procedures, and is expected to revolutionize the future of human-tech or computer interaction seamles\n",
      "8 min read\n",
      "Large Language Models (LLMs) vs Transformers\n",
      "In recent years, advancements in artificial intelligence have led to the development of sophisticated models that are capable of understanding and generating human-like text. Two of the most significant innovations in this space are Large Language Models (LLMs) and Transformers. While they are often discussed together, they serve different purposes\n",
      "7 min read\n",
      "Top  20  Applications of Large Language Models in Real-Life\n",
      "Language models (LLMs), such as GPT-4, have revolutionized numerous industries by leveraging their advanced capabilities in natural language processing (NLP) to enhance efficiency, accuracy, and user experience. From automating tasks to providing personalized services, these models have become indispensable tools across various sectors. This articl\n",
      "8 min read\n",
      "From GPT-3 to GPT-4: Evolution and Innovations in Large Language Models\n",
      "The progression of artificial intelligence (AI) in recent years has been nothing short of extraordinary, with significant strides particularly evident in the realm of natural language processing (NLP). Central to this advancement is the development of large language models (LLMs) like OpenAI's GPT series. This article explores the evolution from GP\n",
      "7 min read\n",
      "10 Free Resources to Learn Large Language Models (LLMs)\n",
      "Large Language Models (LLMs), such as GPT-3 and its successors, are reshaping how we interact with technology. From generating text to answering questions, LLMs are becoming an integral part of many applications. If you're eager to dive into the world of LLMs but don't know where to start, this guide provides 10 Free Resources to Learn Large Langua\n",
      "6 min read\n",
      "Exploring Generative Models: Applications, Examples, and Key Concepts\n",
      "A generative model is a type of machine learning model that aims to learn underlying patterns or distributions of data to generate new, similar data. This is used in unsupervised machine learning to describe phenomena in data, enabling computers to understand the real world. In this article, we will discuss some applications and examples of generat\n",
      "9 min read\n",
      "Rabbit AI: Large Action Models (LAMs)\n",
      "The Large Action Models (LAMs) are advanced artificial intelligence systems that are capable of understanding the human intention and predicting actions. In this article, we will be covering the fundamentals, working and architecture of the Large Action Models. We have all heard about Generative AI & LLMs, used them, and seen their tremendous i\n",
      "9 min read\n",
      "Microsoftâ€™s Large Action Models (LAM): Features, Applications, and Future\n",
      "Microsoft has unveiled its latest innovation in artificial intelligence, Large Action Models (LAM), which go beyond traditional language models to tackle complex tasks with a focus on automation and optimization. Unlike Large Language Models, which prioritize understanding and generating human-like text, LAM is designed for actionable decision-maki\n",
      "9 min read\n",
      "Develop an LLM Application using Openai\n",
      "Language Models (LMs) play a crucial role in natural language processing applications, enabling the development of tools that generate human-like text. OpenAI's Generative Pre-Trained Transformer (GPT) models, such as GPT-3.5-turbo, are widely used in this domain. They excel in understanding context, learning language patterns, and generating coher\n",
      "5 min read\n",
      "Can LLM replace Data Analyst\n",
      "As we know, today's era is all about data, as the quantity of data is increasing daily. Data analysis is the process of extracting, cleaning, and preprocessing the data and gathering insights from the data. Nowadays, there is also a trend of large language models such as ChatGPT4, so many business analysts use large language models to solve their p\n",
      "7 min read\n",
      "Falcon LLM: Comprehensive Guide\n",
      "Falcon LLM is a large language model that is engineered to comprehend and generate human like text, showcasing remarkable improvements in natural language and generation capabilities. This article covers the fundamentals of Falcon LLM and demonstrates how can we perform text generation using Falcon LLM. Table of Content What is Falcon LLM? Key Feat\n",
      "11 min read\n",
      "Pi: World's Friendliest Chatbot Gets Even Smarter with Inflection-2.5 LLM\n",
      "The world of conversational AI, powered by large language models (LLMs), is rapidly evolving. Inflection AI, a leader in this field, is pushing the boundaries with its innovative chatbot, Pi. Known for its friendly personality, Pi is about to get even smarter thanks to a groundbreaking upgrade â€“ the Inflection-2.5 LLM. This cutting-edge AI technolo\n",
      "5 min read\n",
      "RAG Vs Fine-Tuning for Enhancing LLM Performance\n",
      "Data Science and Machine Learning researchers and practitioners alike are constantly exploring innovative strategies to enhance the capabilities of language models. Among the myriad approaches, two prominent techniques have emerged which are Retrieval-Augmented Generation (RAG) and Fine-tuning. The article aims to explore the importance of model pe\n",
      "9 min read\n",
      "NLP vs LLM: Understanding Key Differences\n",
      "In the rapidly evolving field of artificial intelligence, two concepts that often come into focus are Natural Language Processing (NLP) and Large Language Models (LLM). Although they are intertwined, each plays a distinct role in how machines understand and generate human language. This article delves into the definitions, differences, and intercon\n",
      "6 min read\n",
      "Securing LLM Systems Against Prompt Injection\n",
      "Large Language Models (LLMs) have revolutionized the field of artificial intelligence, enabling applications such as chatbots, content generators, and personal assistants. However, the integration of LLMs into various applications has introduced new security vulnerabilities, notably prompt injection attacks. These attacks exploit the way LLMs proce\n",
      "11 min read\n",
      "Prompt Injection in LLM\n",
      "Prompt injection is a significant and emerging concern in the field of artificial intelligence, particularly in the context of large language models (LLMs). As these models become increasingly sophisticated and integrated into various applications, understanding and mitigating prompt injection becomes essential. This article delves into the intrica\n",
      "8 min read\n",
      "Open Sorce LLM : Hugging Face Impacts on the AI Community\n",
      "Hugging Face, a leading force in natural language processing (NLP), has made a profound impact on the AI community through its commitment to open-source principles. By offering advanced NLP models and tools for free, Hugging Face has reshaped the landscape of AI research and development. This article explores how Hugging Faceâ€™s open-source approach\n",
      "4 min read\n",
      "How to Fine-Tune an LLM from Hugging Face\n",
      "Large Language Models (LLMs) have transformed different tasks in natural language processing (NLP) such as translation, summarization, and text generation. Hugging Face's Transformers library offers a wide range of pre-trained models that can be customized for specific purposes through fine-tuning. Adjusting an LLM with task-specific data through f\n",
      "5 min read\n",
      "Explained LLM Leaderboard - 2024\n",
      "Artificial intelligence (AI) has expanded quickly, and there are great improvements in the use of large language models (LLMs). As these models advance, there is a shortage of a good system for evaluating and rating them appropriately. In this respect, the proposed LLM Leaderboard 2024 fulfils the criteria mentioned above by providing a set of stan\n",
      "10 min read\n",
      "Introduction to NExT-GPT: Any-to-Any Multimodal LLM\n",
      "The field of artificial intelligence (AI) has seen rapid advancements in recent years, particularly in the development of large language models (LLMs) like GPT-4. These models have primarily focused on text-based tasks, excelling in natural language understanding and generation. However, as multimodal applicationsâ€”those that involve various forms o\n",
      "6 min read\n",
      "What is JanitorLLM : The Free Version LLM of Janitor AI 2025\n",
      "In an era where Artificial Intelligence and Natural Language Processing (NLP) are transforming industries, JanitorLLM stands out as a prominent example of a state-of-the-art language model that is accessible for free. Developed by Janitor AI, JanitorLLM is designed to offer advanced text generation, comprehension, and interaction capabilities witho\n",
      "6 min read\n",
      "What is LLM Distillation?\n",
      "The size and computational demands of Large Language Models (LLMs) have made them impractical for many real-world applications, especially on edge devices or resource-constrained environments. This is where LLM Distillation comes into play. LLM Distillation is a specialized form of Knowledge Distillation (KD) aimed at compressing large-scale langua\n",
      "5 min read\n",
      "Like\n",
      "34k+ interested Geeks\n",
      "Mastering Generative AI and ChatGPT\n",
      "Explore\n",
      "1k+ interested Geeks\n",
      "Artificial Intelligence for Kids - Complete AI Course for Beginners\n",
      "Explore\n",
      "1k+ interested Geeks\n",
      "Complete TensorFlow Course\n",
      "Explore\n",
      "Corporate & Communications Address:\n",
      "A-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305)\n",
      "Registered Address:\n",
      "K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305\n",
      "Advertise with us\n",
      "Company\n",
      "About Us\n",
      "Legal\n",
      "Privacy Policy\n",
      "Careers\n",
      "In Media\n",
      "Contact Us\n",
      "GFG Corporate Solution\n",
      "Placement Training Program\n",
      "Explore\n",
      "Job-A-Thon Hiring Challenge\n",
      "Hack-A-Thon\n",
      "GfG Weekly Contest\n",
      "Offline Classes (Delhi/NCR)\n",
      "DSA in JAVA/C++\n",
      "Master System Design\n",
      "Master CP\n",
      "GeeksforGeeks Videos\n",
      "Geeks Community\n",
      "Languages\n",
      "Python\n",
      "Java\n",
      "C++\n",
      "PHP\n",
      "GoLang\n",
      "SQL\n",
      "R Language\n",
      "Android Tutorial\n",
      "DSA\n",
      "Data Structures\n",
      "Algorithms\n",
      "DSA for Beginners\n",
      "Basic DSA Problems\n",
      "DSA Roadmap\n",
      "DSA Interview Questions\n",
      "Competitive Programming\n",
      "Data Science & ML\n",
      "Data Science With Python\n",
      "Data Science For Beginner\n",
      "Machine Learning\n",
      "ML Maths\n",
      "Data Visualisation\n",
      "Pandas\n",
      "NumPy\n",
      "NLP\n",
      "Deep Learning\n",
      "Web Technologies\n",
      "HTML\n",
      "CSS\n",
      "JavaScript\n",
      "TypeScript\n",
      "ReactJS\n",
      "NextJS\n",
      "NodeJs\n",
      "Bootstrap\n",
      "Tailwind CSS\n",
      "Python Tutorial\n",
      "Python Programming Examples\n",
      "Django Tutorial\n",
      "Python Projects\n",
      "Python Tkinter\n",
      "Web Scraping\n",
      "OpenCV Tutorial\n",
      "Python Interview Question\n",
      "Computer Science\n",
      "GATE CS Notes\n",
      "Operating Systems\n",
      "Computer Network\n",
      "Database Management System\n",
      "Software Engineering\n",
      "Digital Logic Design\n",
      "Engineering Maths\n",
      "DevOps\n",
      "Git\n",
      "AWS\n",
      "Docker\n",
      "Kubernetes\n",
      "Azure\n",
      "GCP\n",
      "DevOps Roadmap\n",
      "System Design\n",
      "High Level Design\n",
      "Low Level Design\n",
      "UML Diagrams\n",
      "Interview Guide\n",
      "Design Patterns\n",
      "OOAD\n",
      "System Design Bootcamp\n",
      "Interview Questions\n",
      "School Subjects\n",
      "Mathematics\n",
      "Physics\n",
      "Chemistry\n",
      "Biology\n",
      "Social Science\n",
      "English Grammar\n",
      "Commerce\n",
      "Accountancy\n",
      "Business Studies\n",
      "Economics\n",
      "Management\n",
      "HR Management\n",
      "Finance\n",
      "Income Tax\n",
      "Databases\n",
      "SQL\n",
      "MYSQL\n",
      "PostgreSQL\n",
      "PL/SQL\n",
      "MongoDB\n",
      "Preparation Corner\n",
      "Company-Wise Recruitment Process\n",
      "Resume Templates\n",
      "Aptitude Preparation\n",
      "Puzzles\n",
      "Company-Wise Preparation\n",
      "Companies\n",
      "Colleges\n",
      "Competitive Exams\n",
      "JEE Advanced\n",
      "UGC NET\n",
      "UPSC\n",
      "SSC CGL\n",
      "SBI PO\n",
      "SBI Clerk\n",
      "IBPS PO\n",
      "IBPS Clerk\n",
      "More Tutorials\n",
      "Software Development\n",
      "Software Testing\n",
      "Product Management\n",
      "Project Management\n",
      "Linux\n",
      "Excel\n",
      "All Cheat Sheets\n",
      "Recent Articles\n",
      "Free Online Tools\n",
      "Typing Test\n",
      "Image Editor\n",
      "Code Formatters\n",
      "Code Converters\n",
      "Currency Converter\n",
      "Random Number Generator\n",
      "Random Password Generator\n",
      "Write & Earn\n",
      "Write an Article\n",
      "Improve an Article\n",
      "Pick Topics to Write\n",
      "Share your Experiences\n",
      "Internships\n",
      "DSA/\n",
      "Placements\n",
      "DSA - Self Paced Course\n",
      "DSA in JavaScript - Self Paced Course\n",
      "DSA in Python - Self Paced\n",
      "C Programming Course Online - Learn C with Data Structures\n",
      "Complete Interview Preparation\n",
      "Master Competitive Programming\n",
      "Core CS Subject for Interview Preparation\n",
      "Mastering System Design: LLD to HLD\n",
      "Tech Interview 101 - From DSA to System Design [LIVE]\n",
      "DSA to Development [HYBRID]\n",
      "Placement Preparation Crash Course [LIVE]\n",
      "Development/\n",
      "Testing\n",
      "JavaScript Full Course\n",
      "React JS Course\n",
      "React Native Course\n",
      "Django Web Development Course\n",
      "Complete Bootstrap Course\n",
      "Full Stack Development - [LIVE]\n",
      "JAVA Backend Development - [LIVE]\n",
      "Complete Software Testing Course [LIVE]\n",
      "Android Mastery with Kotlin [LIVE]\n",
      "Machine Learning/\n",
      "Data Science\n",
      "Complete Machine Learning & Data Science Program - [LIVE]\n",
      "Data Analytics Training using Excel, SQL, Python & PowerBI - [LIVE]\n",
      "Data Science Training Program - [LIVE]\n",
      "Mastering Generative AI and ChatGPT\n",
      "Data Science Course with IBM Certification\n",
      "Programming Languages\n",
      "C Programming with Data Structures\n",
      "C++ Programming Course\n",
      "Java Programming Course\n",
      "Python Full Course\n",
      "Clouds/\n",
      "Devops\n",
      "DevOps Engineering\n",
      "AWS Solutions Architect Certification\n",
      "Salesforce Certified Administrator Course\n",
      "GATE\n",
      "GATE CS & IT Test Series - 2025\n",
      "GATE DA Test Series 2025\n",
      "GATE CS & IT Course - 2025\n",
      "GATE DA Course 2025\n",
      "GATE Rank Predictor\n",
      "@GeeksforGeeks, Sanchhaya Education Private Limited\n",
      ",\n",
      "All rights reserved\n",
      "We use cookies to ensure you have the best browsing experience on our website. By using our site, you\n",
      "        acknowledge that you have read and understood our\n",
      "Cookie Policy\n",
      "&\n",
      "Privacy Policy\n",
      "Got It !\n",
      "Improvement\n",
      "Suggest changes\n",
      "Suggest Changes\n",
      "Help us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\n",
      "Create Improvement\n",
      "Enhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\n",
      "Suggest Changes\n",
      "min 4 words, max CharLimit:2000\n",
      "What kind of Experience do you want to share?\n",
      "Interview Experiences\n",
      "Admission Experiences\n",
      "Career Journeys\n",
      "Work Experiences\n",
      "Campus Experiences\n",
      "Competitive Exam Experiences\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt_for(web))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a7950bb-7c89-4f08-b1d3-7ff8b3269bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_for(website):\n",
    "    return [\n",
    "        {\"role\":\"system\", \"content\": system_prompt},\n",
    "        {\"role\":\"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae686ad4-b6dc-47e0-bd50-024d21ab65aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an assistant that analyzes the contents of a website and provides a short summary, ignoring text that might be navigation related. Respond in markdown.'},\n",
       " {'role': 'user',\n",
       "  'content': \"You are looking at a website titled LLM Architecture: Exploring the Technical Architecture Behind Large Language Models - GeeksforGeeks\\nThe content of this website are as follows;     please provide a short summary of this website in markdown.    If it included news or announcements, then summarize these too.\\n\\nSkip to content\\nCourses\\nGet IBM Certifications\\nComplete Machine Learning & Data Science Program\\nData Science Training Program\\nData Analytics Training using Excel, SQL, Python & PowerBI\\nComplete Data Analytics Program\\nDSA to Development\\nFor Working Professionals\\nData Structure & Algorithm Classes (Live)\\nSystem Design (Live)\\nJAVA Backend Development(Live)\\nDevOps(Live)\\nData Structures & Algorithms in Python\\nFor Students\\nInterview Preparation Course\\nGATE CS & IT 2025\\nData Science (Live)\\nData Structure & Algorithm-Self Paced(C++/JAVA)\\nMaster Competitive Programming(Live)\\nFull Stack Development with React & Node JS(Live)\\nAll Courses\\nFull Stack Development\\nData Science Program\\nTutorials\\nPython\\nPython Tutorial\\nPython Programs\\nAdvanced Python Tutorial\\nPython Projects\\nWeb Development in Python\\nDjango\\nFlask\\nPostman\\nWeb Scrapping in Python\\nData Structures & Algorithms\\nComplete DSA Tutorial\\nCompany Wise Preparation\\nCompetitive Programming\\nSDE Sheets\\nLanguages\\nJava\\nC++\\nR Tutorial\\nC\\nC#\\nSQL\\nPerl\\nGo Language\\nInterview Corner\\nSystem Design Tutorial\\nTop Topics\\nPractice Company Questions\\nInterview Experiences\\nExperienced Interviews\\nInternship Interviews\\nMultiple Choice Quizzes\\nAptitude for Placements\\nPuzzles for Interviews\\nWeb Development\\nHTML\\nCSS\\nJavaScript\\nTypeScript\\nReactJS\\nNextJS\\nNode.js\\nPHP\\n100 Days of Web Development\\nCS Subjects\\nEngineering Mathematics\\nOperating System\\nDBMS\\nComputer Networks\\nComputer Organization and Architecture\\nTheory of Computation\\nCompiler Design\\nDigital Logic\\nSoftware Engineering\\nDevOps And Linux\\nDevOps Tutorial\\nGIT\\nAWS\\nKubernetes\\nDocker\\nMicrosoft Azure Tutorial\\nGoogle Cloud Platform\\nLinux Tutorial\\nGATE\\nGATE Computer Science Notes\\nGATE CS Original Papers and Official Keys\\nGATE CS 2025 Syllabus\\nGATE DA 2025 Syllabus\\nSchool Learning\\nGeeksforGeeks Videos\\nData Science\\nMachine Learning\\nEDA [Exploratory Data Analysis]\\nData Analysis with Python\\nPython Data Visualization\\nData Science using Python\\nData Science using R\\nDeep Learning\\nNLP Tutorial\\nComputer Vision\\nInterview Corner\\nMachine Learning Interview Question\\nDeep Learning Interview Question\\nNLP Interview Question\\nPython Interview Questions\\nTop 50 R Interview Questions\\nPractice\\nAll DSA Problems\\nProblem of the Day\\nGfG SDE Sheet\\nCurated DSA Lists\\nBeginner's DSA Sheet\\nLove Babbar Sheet\\nTop 50 Array Problems\\nTop 50 String Problems\\nTop 50 Tree Problems\\nTop 50 Graph Problems\\nTop 50 DP Problems\\nNotifications\\nMark all as read\\nAll\\nView All\\nNotifications\\nMark all as read\\nAll\\nUnread\\nRead\\nYou're all caught up!!\\nData Science IBM Certification\\nData Science\\nData Science Projects\\nData Analysis\\nData Visualization\\nMachine Learning\\nML Projects\\nDeep Learning\\nNLP\\nComputer Vision\\nArtificial Intelligence\\nâ–²\\nOpen In App\\nGfG 160\\nShare Your Experiences\\nLLM Architecture: Exploring the Technical Architecture Behind Large Language Models\\nExploring Multimodal Large Language Models\\nLLMs vs. SLMs : Comparative Analysis of Language Model Architectures\\nWhat is a Large Language Model (LLM)\\nLLM vs GPT : Comparing Large Language Models and GPT\\n10 Exciting Project Ideas Using Large Language Models (LLMs)\\nDiscounting Techniques in Language Models\\nFine Tuning Large Language Model (LLM)\\nMultiturn Deviation in Large Language Model\\n10 Free Resources to Learn Large Language Models (LLMs)\\nEnhancing Natural Language Processing with Transfer Learning: Techniques, Models, and Applications\\nGemma vs. Gemini vs. LLM (Large Language Model)\\nFuture of Large Language Models\\nOllama Explained: Transforming AI Accessibility and Language Processing\\nExploring Llama-3-8b-Instruct: Advancements, Applications, and Future Prospects in AI Language Models\\nFrom GPT-3 to GPT-4: Evolution and Innovations in Large Language Models\\nWhat is PaLM 2: Google's Large Language Model Explained\\nAdvanced Smoothing Techniques in Language Models\\nTop 20 LLM (Large Language Models)\\nELIZA: The First Step in Human-Computer Interaction Through Natural Language Processing\\nDSA to Development\\nCourse\\nLLM Architecture: Exploring the Technical Architecture Behind Large Language Models\\nLast Updated :\\n20 Sep, 2024\\nSummarize\\nComments\\nImprove\\nSuggest changes\\nLike Article\\nLike\\nShare\\nReport\\nFollow\\nLarge Language Models (LLMs)\\nhave become a cornerstone in the field of artificial intelligence, driving advancements in natural language processing (NLP), conversational AI, and various applications that require understanding and generating human-like text. The technical architecture of these models is a complex interplay of several components, each designed to maximize performance, scalability, and accuracy.\\nExploring the Technical Architecture Behind Large Language Models\\nIn this article, we will delve into the\\nkey aspects of the technical architecture of LLMs, exploring their structure, training processes, and the innovations that power them.\\nTable of Content\\nIntroduction to Large Language Model Architecture\\nArchitecture of Large Language Models (LLMs)\\nInput Layer: Tokenization\\nEmbedding Layer\\nTransformer Architecture\\nStacking Layers\\nOutput Layer: Decoding\\nOptimization\\nScaling and Parallelism\\nEthical Considerations\\nIntroduction to Large Language Model Architecture\\nLarge Language Models\\nare designed to understand and generate human language. Modern language models, particularly those built on transformer architectures, have revolutionized the field with their ability to process and generate text with high accuracy and relevance. The technical architecture of these models is both complex and fascinating, involving several key components and mechanisms.\\nArchitecture of Large Language Models (LLMs)\\nArchitecture of Large Language Models (LLMs)\\nLarge Language Models (LLMs)\\nlike GPT-4, BERT, and others are complex systems designed to process and generate human-like text. Their architecture involves multiple layers and components, each contributing to the model's ability to understand and produce language. Here's an overview of the\\nkey components and the architecture of LLMs\\n:\\nInput Layer: Tokenization\\nTokenization:\\nThe input text is broken down into smaller units called tokens, which can be words, subwords, or characters. These tokens are then converted into numerical representations (embeddings) that the model can process.\\nEmbedding Layer\\nWord Embeddings:\\nEach token is mapped to a dense vector in a high-dimensional space, representing its semantic meaning. Common techniques include Word2Vec, GloVe, and embeddings learned during model training.\\nPositional Embeddings:\\nSince transformers do not inherently understand the order of tokens, positional embeddings are added to the word embeddings to give the model information about the token positions within a sentence.\\nTransformer Architecture\\nSelf-Attention Mechanism:\\nAttention Scores:\\nThe self-attention mechanism computes a set of attention scores that determine how much focus each word should give to other words in the sequence.\\nQuery, Key, and Value (Q, K, V):\\nThese are linear projections of the input embeddings used to compute attention. The model calculates the relevance of each token to others using the dot product of Query and Key vectors, followed by a softmax operation to obtain attention weights. The Value vectors are then weighted by these attention scores.\\nMulti-Head Attention:\\nMultiple attention heads are used to capture different aspects of the relationships between tokens. Each head operates in a separate subspace, and the results are concatenated and projected back into the original space.\\nFeedforward\\nNeural Network\\n:\\nAfter the attention mechanism, the output is passed through a feedforward neural network (a series of dense layers with activation functions), applied independently to each position.\\nLayer Normalization and Residual Connections:\\nEach sub-layer (attention and feedforward) is followed by layer normalization and a residual connection, which helps stabilize training and allows for deeper networks.\\nStacking Layers\\nTransformer Blocks:\\nThe architecture typically involves stacking multiple transformer layers (or blocks) on top of each other. Each block consists of a multi-head self-attention mechanism and a feedforward neural network. This stacking allows the model to learn complex hierarchical representations of the data.\\nOutput Layer: Decoding\\nLanguage Modeling Objective:\\nIn autoregressive models like GPT, the model is trained to predict the next token in a sequence given the previous tokens. In masked language models like BERT, the model predicts missing tokens in a sequence.\\nSoftmax Layer:\\nThe final layer is typically a softmax function that converts the model's output into a probability distribution over the vocabulary, allowing it to select the most likely next token or fill in a masked token.\\nTraining and Fine-Tuning\\nPre-Training:\\nData Collection:\\nLarge-scale language models are pre-trained on diverse and extensive datasets that include books, articles, websites, and other text sources. This helps the model learn a broad understanding of language and context.\\nObjective Functions:\\nDuring pre-training, models typically use objective functions such as masked language modeling (MLM) or autoregressive modeling. MLM involves predicting missing words in a sentence, while autoregressive modeling focuses on predicting the next word in a sequence.\\nComputational Resources:\\nTraining these models requires powerful hardware, including GPUs or TPUs, and substantial memory. Distributed computing and parallel processing are often employed to handle the computational demands.\\nFine-Tuning:\\nDomain-Specific Training:\\nAfter pre-training, models are fine-tuned on specific tasks or domains. Fine-tuning involves additional training on more specialized datasets to adapt the model to particular applications, such as sentiment analysis or machine translation.\\nHyperparameter Tuning\\n:\\nFine-tuning also involves adjusting hyperparameters, such as learning rates and batch sizes, to optimize the model's performance for the target task.\\nOptimization\\nLoss Function\\n:\\nThe model is trained to minimize a loss function, typically cross-entropy loss, which measures the difference between the predicted probability distribution and the actual distribution.\\nGradient Descent and Backpropagation:\\nThe model's parameters are updated using gradient descent and backpropagation to minimize the loss function across many iterations.\\nScaling and Parallelism\\nModel Scaling:\\nModern LLMs often contain billions of parameters, requiring significant computational resources. Techniques like model parallelism, data parallelism, and distributed training are used to handle the computational load.\\nInference Optimization:\\nTo make LLMs efficient at inference time, techniques like quantization, pruning, and distillation are often used to reduce model size and improve speed without significantly compromising performance.\\nEthical Considerations\\nBias and Fairness:\\nLLMs can inherit biases present in the training data, leading to problematic outputs. Addressing these issues involves careful dataset curation, bias mitigation techniques, and ongoing monitoring.\\nSafety and Robustness:\\nEnsuring that LLMs produce safe and reliable outputs, especially in sensitive applications, is a critical concern. This involves implementing safeguards against harmful content and adversarial attacks.\\nConclusion\\nThe technical architecture behind modern language models is a marvel of engineering and innovation. The transformer architecture, with its self-attention mechanisms, positional encoding, and multi-head attention, has set the foundation for the remarkable capabilities of these models. Advances in training techniques, fine-tuning strategies, and architectural innovations continue to drive the evolution of language models, making them more powerful, efficient, and versatile.\\nAs language models become increasingly sophisticated, understanding their architecture helps us appreciate the complexity and potential of these technologies. Whether you're a researcher, developer, or enthusiast, exploring the technical details of language models offers valuable insights into how these AI systems are shaping the future of human-computer interaction and natural language understanding.\\nGet IBM Certification\\nand a\\n90% fee refund\\non completing 90% course in 90 days!\\nTake the Three 90 Challenge today.\\nMaster Machine Learning, Data Science & AI with this complete program and also get a 90% refund. What more motivation do you need?\\nStart the challenge right away!\\nComment\\nMore info\\nAdvertise with us\\nNext Article\\nExploring Multimodal Large Language Models\\nP\\npoonamvbo5\\nFollow\\nImprove\\nArticle Tags :\\nAI-ML-DS Blogs\\nAI-ML-DS\\nAI Blogs\\nSimilar Reads\\nTop 20 LLM (Large Language Models)\\nLarge Language Model commonly known as an LLM, refers to a neural network equipped with billions of parameters and trained extensively on extensive datasets of unlabeled text. This training typically involves self-supervised or semi-supervised learning techniques. In this article, we explore about Top 20 LLM Models and get to know how each model ha\\n15+ min read\\nLLM vs GPT : Comparing Large Language Models and GPT\\nIn recent years, the field of natural language processing (NLP) has made tremendous strides, largely due to the development of large language models (LLMs) and, more specifically, the Generative Pre-trained Transformer (GPT) series. Both LLMs and GPTs have transformed how machines understand and generate human language. Table of Content What is a L\\n4 min read\\nExploring Multimodal Large Language Models\\nMultimodal large language models (LLMs) integrate and process diverse types of data (such as text, images, audio, and video) to enhance understanding and generate comprehensive responses. The article aims to explore the evolution, components, importance, and examples of multimodal large language models (LLMs) integrating text, images, audio, and vi\\n8 min read\\nGemma vs. Gemini vs. LLM (Large Language Model)\\nArtificial Intelligence (AI) has witnessed exponential growth, with language models at the forefront of many transformative applications. Three key players in this space, Gemma, Gemini, and LLMs (Large Language Models), represent cutting-edge advancements in AI-driven conversational agents and data processing. While they share common goals, these t\\n5 min read\\nFine Tuning Large Language Model (LLM)\\nLarge Language Models (LLMs) have dramatically transformed natural language processing (NLP), excelling in tasks like text generation, translation, summarization, and question-answering. However, these models may not always be ideal for specific domains or tasks. To address this, fine-tuning is performed. Fine-tuning customizes pre-trained LLMs to\\n13 min read\\nWhat is a Large Language Model (LLM)\\nLarge Language Models (LLMs) represent a breakthrough in artificial intelligence, employing neural network techniques with extensive parameters for advanced language processing. This article explores the evolution, architecture, applications, and challenges of LLMs, focusing on their impact in the field of Natural Language Processing (NLP). What ar\\n10 min read\\nExploring Llama-3-8b-Instruct: Advancements, Applications, and Future Prospects in AI Language Models\\nIn the rapidly evolving landscape of artificial intelligence, language models play a crucial role in driving innovation and transforming various industries. One of the latest advancements in this domain is the Llama-3-8b-Instruct model, a powerful AI tool designed to enhance natural language understanding and generation. This article delves into th\\n7 min read\\nBuild RAG pipeline using Open Source Large Language Models\\nIn this article, we will implement Retrieval Augmented Generation aka RAG pipeline using Open-Source Large Language models with Langchain and HuggingFace. Open Source LLMs vs Closed Source LLMsLarge Language models are all over the place. Because of the rise of Large Language models, AI came into the limelight in the market. From development to the\\n8 min read\\nFuture of Large Language Models\\nIn the last few years, the development of artificial intelligence has been in significant demand, with the emergence of Large Language Models (LLMs). This streamlined model entails advanced machine learning methods, has transformed natural language procedures, and is expected to revolutionize the future of human-tech or computer interaction seamles\\n8 min read\\nLarge Language Models (LLMs) vs Transformers\\nIn recent years, advancements in artificial intelligence have led to the development of sophisticated models that are capable of understanding and generating human-like text. Two of the most significant innovations in this space are Large Language Models (LLMs) and Transformers. While they are often discussed together, they serve different purposes\\n7 min read\\nTop  20  Applications of Large Language Models in Real-Life\\nLanguage models (LLMs), such as GPT-4, have revolutionized numerous industries by leveraging their advanced capabilities in natural language processing (NLP) to enhance efficiency, accuracy, and user experience. From automating tasks to providing personalized services, these models have become indispensable tools across various sectors. This articl\\n8 min read\\nFrom GPT-3 to GPT-4: Evolution and Innovations in Large Language Models\\nThe progression of artificial intelligence (AI) in recent years has been nothing short of extraordinary, with significant strides particularly evident in the realm of natural language processing (NLP). Central to this advancement is the development of large language models (LLMs) like OpenAI's GPT series. This article explores the evolution from GP\\n7 min read\\n10 Free Resources to Learn Large Language Models (LLMs)\\nLarge Language Models (LLMs), such as GPT-3 and its successors, are reshaping how we interact with technology. From generating text to answering questions, LLMs are becoming an integral part of many applications. If you're eager to dive into the world of LLMs but don't know where to start, this guide provides 10 Free Resources to Learn Large Langua\\n6 min read\\nExploring Generative Models: Applications, Examples, and Key Concepts\\nA generative model is a type of machine learning model that aims to learn underlying patterns or distributions of data to generate new, similar data. This is used in unsupervised machine learning to describe phenomena in data, enabling computers to understand the real world. In this article, we will discuss some applications and examples of generat\\n9 min read\\nRabbit AI: Large Action Models (LAMs)\\nThe Large Action Models (LAMs) are advanced artificial intelligence systems that are capable of understanding the human intention and predicting actions. In this article, we will be covering the fundamentals, working and architecture of the Large Action Models. We have all heard about Generative AI & LLMs, used them, and seen their tremendous i\\n9 min read\\nMicrosoftâ€™s Large Action Models (LAM): Features, Applications, and Future\\nMicrosoft has unveiled its latest innovation in artificial intelligence, Large Action Models (LAM), which go beyond traditional language models to tackle complex tasks with a focus on automation and optimization. Unlike Large Language Models, which prioritize understanding and generating human-like text, LAM is designed for actionable decision-maki\\n9 min read\\nDevelop an LLM Application using Openai\\nLanguage Models (LMs) play a crucial role in natural language processing applications, enabling the development of tools that generate human-like text. OpenAI's Generative Pre-Trained Transformer (GPT) models, such as GPT-3.5-turbo, are widely used in this domain. They excel in understanding context, learning language patterns, and generating coher\\n5 min read\\nCan LLM replace Data Analyst\\nAs we know, today's era is all about data, as the quantity of data is increasing daily. Data analysis is the process of extracting, cleaning, and preprocessing the data and gathering insights from the data. Nowadays, there is also a trend of large language models such as ChatGPT4, so many business analysts use large language models to solve their p\\n7 min read\\nFalcon LLM: Comprehensive Guide\\nFalcon LLM is a large language model that is engineered to comprehend and generate human like text, showcasing remarkable improvements in natural language and generation capabilities. This article covers the fundamentals of Falcon LLM and demonstrates how can we perform text generation using Falcon LLM. Table of Content What is Falcon LLM? Key Feat\\n11 min read\\nPi: World's Friendliest Chatbot Gets Even Smarter with Inflection-2.5 LLM\\nThe world of conversational AI, powered by large language models (LLMs), is rapidly evolving. Inflection AI, a leader in this field, is pushing the boundaries with its innovative chatbot, Pi. Known for its friendly personality, Pi is about to get even smarter thanks to a groundbreaking upgrade â€“ the Inflection-2.5 LLM. This cutting-edge AI technolo\\n5 min read\\nRAG Vs Fine-Tuning for Enhancing LLM Performance\\nData Science and Machine Learning researchers and practitioners alike are constantly exploring innovative strategies to enhance the capabilities of language models. Among the myriad approaches, two prominent techniques have emerged which are Retrieval-Augmented Generation (RAG) and Fine-tuning. The article aims to explore the importance of model pe\\n9 min read\\nNLP vs LLM: Understanding Key Differences\\nIn the rapidly evolving field of artificial intelligence, two concepts that often come into focus are Natural Language Processing (NLP) and Large Language Models (LLM). Although they are intertwined, each plays a distinct role in how machines understand and generate human language. This article delves into the definitions, differences, and intercon\\n6 min read\\nSecuring LLM Systems Against Prompt Injection\\nLarge Language Models (LLMs) have revolutionized the field of artificial intelligence, enabling applications such as chatbots, content generators, and personal assistants. However, the integration of LLMs into various applications has introduced new security vulnerabilities, notably prompt injection attacks. These attacks exploit the way LLMs proce\\n11 min read\\nPrompt Injection in LLM\\nPrompt injection is a significant and emerging concern in the field of artificial intelligence, particularly in the context of large language models (LLMs). As these models become increasingly sophisticated and integrated into various applications, understanding and mitigating prompt injection becomes essential. This article delves into the intrica\\n8 min read\\nOpen Sorce LLM : Hugging Face Impacts on the AI Community\\nHugging Face, a leading force in natural language processing (NLP), has made a profound impact on the AI community through its commitment to open-source principles. By offering advanced NLP models and tools for free, Hugging Face has reshaped the landscape of AI research and development. This article explores how Hugging Faceâ€™s open-source approach\\n4 min read\\nHow to Fine-Tune an LLM from Hugging Face\\nLarge Language Models (LLMs) have transformed different tasks in natural language processing (NLP) such as translation, summarization, and text generation. Hugging Face's Transformers library offers a wide range of pre-trained models that can be customized for specific purposes through fine-tuning. Adjusting an LLM with task-specific data through f\\n5 min read\\nExplained LLM Leaderboard - 2024\\nArtificial intelligence (AI) has expanded quickly, and there are great improvements in the use of large language models (LLMs). As these models advance, there is a shortage of a good system for evaluating and rating them appropriately. In this respect, the proposed LLM Leaderboard 2024 fulfils the criteria mentioned above by providing a set of stan\\n10 min read\\nIntroduction to NExT-GPT: Any-to-Any Multimodal LLM\\nThe field of artificial intelligence (AI) has seen rapid advancements in recent years, particularly in the development of large language models (LLMs) like GPT-4. These models have primarily focused on text-based tasks, excelling in natural language understanding and generation. However, as multimodal applicationsâ€”those that involve various forms o\\n6 min read\\nWhat is JanitorLLM : The Free Version LLM of Janitor AI 2025\\nIn an era where Artificial Intelligence and Natural Language Processing (NLP) are transforming industries, JanitorLLM stands out as a prominent example of a state-of-the-art language model that is accessible for free. Developed by Janitor AI, JanitorLLM is designed to offer advanced text generation, comprehension, and interaction capabilities witho\\n6 min read\\nWhat is LLM Distillation?\\nThe size and computational demands of Large Language Models (LLMs) have made them impractical for many real-world applications, especially on edge devices or resource-constrained environments. This is where LLM Distillation comes into play. LLM Distillation is a specialized form of Knowledge Distillation (KD) aimed at compressing large-scale langua\\n5 min read\\nLike\\n34k+ interested Geeks\\nMastering Generative AI and ChatGPT\\nExplore\\n1k+ interested Geeks\\nArtificial Intelligence for Kids - Complete AI Course for Beginners\\nExplore\\n1k+ interested Geeks\\nComplete TensorFlow Course\\nExplore\\nCorporate & Communications Address:\\nA-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305)\\nRegistered Address:\\nK 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305\\nAdvertise with us\\nCompany\\nAbout Us\\nLegal\\nPrivacy Policy\\nCareers\\nIn Media\\nContact Us\\nGFG Corporate Solution\\nPlacement Training Program\\nExplore\\nJob-A-Thon Hiring Challenge\\nHack-A-Thon\\nGfG Weekly Contest\\nOffline Classes (Delhi/NCR)\\nDSA in JAVA/C++\\nMaster System Design\\nMaster CP\\nGeeksforGeeks Videos\\nGeeks Community\\nLanguages\\nPython\\nJava\\nC++\\nPHP\\nGoLang\\nSQL\\nR Language\\nAndroid Tutorial\\nDSA\\nData Structures\\nAlgorithms\\nDSA for Beginners\\nBasic DSA Problems\\nDSA Roadmap\\nDSA Interview Questions\\nCompetitive Programming\\nData Science & ML\\nData Science With Python\\nData Science For Beginner\\nMachine Learning\\nML Maths\\nData Visualisation\\nPandas\\nNumPy\\nNLP\\nDeep Learning\\nWeb Technologies\\nHTML\\nCSS\\nJavaScript\\nTypeScript\\nReactJS\\nNextJS\\nNodeJs\\nBootstrap\\nTailwind CSS\\nPython Tutorial\\nPython Programming Examples\\nDjango Tutorial\\nPython Projects\\nPython Tkinter\\nWeb Scraping\\nOpenCV Tutorial\\nPython Interview Question\\nComputer Science\\nGATE CS Notes\\nOperating Systems\\nComputer Network\\nDatabase Management System\\nSoftware Engineering\\nDigital Logic Design\\nEngineering Maths\\nDevOps\\nGit\\nAWS\\nDocker\\nKubernetes\\nAzure\\nGCP\\nDevOps Roadmap\\nSystem Design\\nHigh Level Design\\nLow Level Design\\nUML Diagrams\\nInterview Guide\\nDesign Patterns\\nOOAD\\nSystem Design Bootcamp\\nInterview Questions\\nSchool Subjects\\nMathematics\\nPhysics\\nChemistry\\nBiology\\nSocial Science\\nEnglish Grammar\\nCommerce\\nAccountancy\\nBusiness Studies\\nEconomics\\nManagement\\nHR Management\\nFinance\\nIncome Tax\\nDatabases\\nSQL\\nMYSQL\\nPostgreSQL\\nPL/SQL\\nMongoDB\\nPreparation Corner\\nCompany-Wise Recruitment Process\\nResume Templates\\nAptitude Preparation\\nPuzzles\\nCompany-Wise Preparation\\nCompanies\\nColleges\\nCompetitive Exams\\nJEE Advanced\\nUGC NET\\nUPSC\\nSSC CGL\\nSBI PO\\nSBI Clerk\\nIBPS PO\\nIBPS Clerk\\nMore Tutorials\\nSoftware Development\\nSoftware Testing\\nProduct Management\\nProject Management\\nLinux\\nExcel\\nAll Cheat Sheets\\nRecent Articles\\nFree Online Tools\\nTyping Test\\nImage Editor\\nCode Formatters\\nCode Converters\\nCurrency Converter\\nRandom Number Generator\\nRandom Password Generator\\nWrite & Earn\\nWrite an Article\\nImprove an Article\\nPick Topics to Write\\nShare your Experiences\\nInternships\\nDSA/\\nPlacements\\nDSA - Self Paced Course\\nDSA in JavaScript - Self Paced Course\\nDSA in Python - Self Paced\\nC Programming Course Online - Learn C with Data Structures\\nComplete Interview Preparation\\nMaster Competitive Programming\\nCore CS Subject for Interview Preparation\\nMastering System Design: LLD to HLD\\nTech Interview 101 - From DSA to System Design [LIVE]\\nDSA to Development [HYBRID]\\nPlacement Preparation Crash Course [LIVE]\\nDevelopment/\\nTesting\\nJavaScript Full Course\\nReact JS Course\\nReact Native Course\\nDjango Web Development Course\\nComplete Bootstrap Course\\nFull Stack Development - [LIVE]\\nJAVA Backend Development - [LIVE]\\nComplete Software Testing Course [LIVE]\\nAndroid Mastery with Kotlin [LIVE]\\nMachine Learning/\\nData Science\\nComplete Machine Learning & Data Science Program - [LIVE]\\nData Analytics Training using Excel, SQL, Python & PowerBI - [LIVE]\\nData Science Training Program - [LIVE]\\nMastering Generative AI and ChatGPT\\nData Science Course with IBM Certification\\nProgramming Languages\\nC Programming with Data Structures\\nC++ Programming Course\\nJava Programming Course\\nPython Full Course\\nClouds/\\nDevops\\nDevOps Engineering\\nAWS Solutions Architect Certification\\nSalesforce Certified Administrator Course\\nGATE\\nGATE CS & IT Test Series - 2025\\nGATE DA Test Series 2025\\nGATE CS & IT Course - 2025\\nGATE DA Course 2025\\nGATE Rank Predictor\\n@GeeksforGeeks, Sanchhaya Education Private Limited\\n,\\nAll rights reserved\\nWe use cookies to ensure you have the best browsing experience on our website. By using our site, you\\r\\n        acknowledge that you have read and understood our\\nCookie Policy\\n&\\nPrivacy Policy\\nGot It !\\nImprovement\\nSuggest changes\\nSuggest Changes\\nHelp us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\\nCreate Improvement\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\\nSuggest Changes\\nmin 4 words, max CharLimit:2000\\nWhat kind of Experience do you want to share?\\nInterview Experiences\\nAdmission Experiences\\nCareer Journeys\\nWork Experiences\\nCampus Experiences\\nCompetitive Exam Experiences\"}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_for(web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0893ef15-7cce-4e43-b4bf-7f5c086f0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    reponse = openai.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash-thinking-exp-01-21\",\n",
    "        messages=message_for(website)\n",
    "    )\n",
    "    return reponse.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ff829ea-ced3-419c-9870-3fec33c3e231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This GeeksforGeeks page is an article titled \"LLM Architecture: Exploring the Technical Architecture Behind Large Language Models\".\\n\\nIt provides an overview of the technical architecture of Large Language Models (LLMs), explaining the key components and processes involved.\\n\\nHere\\'s a summary of the article\\'s main points:\\n\\n*   **Introduction to LLM Architecture**: LLMs are crucial for advancements in NLP and AI, particularly in understanding and generating human-like text. Their architecture, based on transformers, is complex and enables high accuracy and relevance.\\n*   **Architecture Components**:\\n    *   **Input Layer (Tokenization)**:  Text is broken into tokens (words, subwords, characters).\\n    *   **Embedding Layer**: Tokens are converted into numerical vectors (embeddings) representing semantic meaning, including positional embeddings to capture word order.\\n    *   **Transformer Architecture**:\\n        *   **Self-Attention Mechanism**:  Calculates attention scores to focus on relevant words using Query, Key, and Value vectors, often implemented as Multi-Head Attention.\\n        *   **Feedforward Neural Network**: Processes attention output with dense layers and activation functions.\\n        *   **Layer Normalization & Residual Connections**: Stabilize training and enable deeper networks.\\n    *   **Stacking Layers**: Multiple transformer blocks are stacked to learn complex language representations.\\n    *   **Output Layer (Decoding)**: Predicts the next token (autoregressive) or masked tokens using a Softmax layer for probability distribution.\\n*   **Training and Fine-Tuning**:\\n    *   **Pre-training**: LLMs are trained on massive text datasets using objectives like Masked Language Modeling (MLM) or autoregressive modeling, requiring significant computational resources.\\n    *   **Fine-tuning**:  Models are adapted for specific tasks or domains with specialized datasets and hyperparameter adjustments.\\n*   **Optimization**: Training minimizes a loss function (cross-entropy) using gradient descent and backpropagation.\\n*   **Scaling and Parallelism**: Techniques like model parallelism and data parallelism are used to handle the billions of parameters in LLMs. Inference optimization methods like quantization and pruning improve efficiency.\\n*   **Ethical Considerations**:  Addresses concerns about bias, fairness, safety, and robustness in LLM outputs.\\n*   **Conclusion**: The transformer architecture is foundational to LLMs\\' capabilities. Ongoing advancements are enhancing their power and versatility, making understanding their architecture crucial for anyone in the AI field.\\n\\nThe article aims to educate readers on the technical intricacies of LLMs and their significance in shaping the future of AI and natural language understanding.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"https://www.geeksforgeeks.org/exploring-the-technical-architecture-behind-large-language-models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a7bb51a-4688-40a9-9d46-9130e8a69cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8121ea1b-a932-4162-8d92-23418f28f827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This GeeksforGeeks page provides an article explaining the technical architecture of Large Language Models (LLMs).\n",
       "\n",
       "The article covers:\n",
       "\n",
       "*   **Introduction to LLM Architecture:**  LLMs are crucial for NLP and AI, built on transformer architectures for processing and generating human-like text.\n",
       "*   **Architecture of LLMs:**\n",
       "    *   **Input Layer (Tokenization & Embedding):**  Text is broken into tokens and converted into numerical embeddings (Word2Vec, GloVe). Positional embeddings are added for sequence order.\n",
       "    *   **Transformer Architecture:**  Utilizes self-attention mechanisms (Query, Key, Value) to weigh word relationships, multi-head attention for capturing diverse relationships, and feedforward neural networks. Layer normalization and residual connections are used for stability.\n",
       "    *   **Stacking Layers:** Multiple transformer blocks create hierarchical representations.\n",
       "    *   **Output Layer (Decoding):** Uses softmax to predict the next token based on probability distribution.\n",
       "*   **Training and Fine-Tuning:**\n",
       "    *   **Pre-training:**  LLMs are pre-trained on massive datasets using objectives like masked language modeling (MLM) or autoregressive modeling, requiring significant computational resources.\n",
       "    *   **Fine-tuning:** Models are adapted for specific tasks with specialized datasets and hyperparameter adjustments.\n",
       "*   **Optimization:**  Loss functions (cross-entropy), gradient descent, and backpropagation are used for training.\n",
       "*   **Scaling and Parallelism:** Techniques like model and data parallelism are used to handle large models and optimize inference speed (quantization, pruning).\n",
       "*   **Ethical Considerations:** Discusses biases in training data and the need for safety measures against harmful content.\n",
       "\n",
       "**Announcement:**\n",
       "\n",
       "*   There is a promotional announcement for an IBM certification program in Data Science, offering a 90% fee refund upon completing 90% of the course in 90 days."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://www.geeksforgeeks.org/exploring-the-technical-architecture-behind-large-language-models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fd25b00-2cb3-46b8-9424-5c3d8a325fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This website is for Anthropic, an AI safety and research company based in San Francisco.\n",
       "\n",
       "It highlights their focus on **AI safety and research**, aiming to create **reliable and beneficial AI systems**. They emphasize putting **safety at the frontier** of their AI research and products.\n",
       "\n",
       "**Key announcements include:**\n",
       "\n",
       "*   **Claude 3.5 Sonnet** is their most intelligent AI model and is now available.\n",
       "*   A **statement from Dario Amodei on the Paris AI Action Summit** is available to read.\n",
       "\n",
       "Anthropic encourages users to:\n",
       "\n",
       "*   **Try Claude** via Claude.ai.\n",
       "*   **Build with Claude** by using their API to create AI-powered applications.\n",
       "\n",
       "They also mention their work in:\n",
       "\n",
       "*   **Policy**: Anthropic Economic Index and Responsible Scaling Policy.\n",
       "*   **Research**: Constitutional AI: Harmlessness from AI Feedback, and Core Views on AI Safety.\n",
       "\n",
       "Anthropic describes their team as interdisciplinary with expertise in ML, physics, policy, and product."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://www.anthropic.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c166082-ba5a-468a-825a-93b8f9a6d4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a summary of the CNN website content:\n",
       "\n",
       "This website is the online platform for CNN, a news organization. It presents a wide range of current events, categorized into sections such as:\n",
       "\n",
       "*   **Top News:**  Breaking stories including updates on the Gaza ceasefire and hostage situation, the death of Shiri Bibas, and political developments in the US involving Donald Trump and the Pentagon.\n",
       "*   **US News:**  Covers US politics, including Trump's activities and internal political issues, as well as other domestic news like California wildfire recovery and legal cases.\n",
       "*   **World News:**  Reports on global events, including the Ukraine-Russia war, the Israel-Hamas conflict, international elections (Germany), and various regional news from Africa, Americas, Asia, Europe, and the Middle East.\n",
       "*   **Business:** Features articles on markets, technology, media, and the economy, including topics like Elon Musk's business ventures, cryptocurrency regulations, and international finance.\n",
       "*   **Health:**  Includes articles on physical and mental health, fitness, food, sleep, and relationships. Recent topics include bird flu concerns and mental health in teens.\n",
       "*   **Entertainment:** Covers movies, television, celebrity news, and technology-related entertainment topics.\n",
       "*   **Style:**  Features articles on arts, design, fashion, architecture, luxury, and beauty.\n",
       "*   **Travel:**  Provides travel news, destination guides, and articles related to food, drink, and accommodations.\n",
       "*   **Sports:**  Reports on various sports including football, tennis, golf, and US sports leagues, with recent highlights on Steph Curry's milestone and boxing news.\n",
       "*   **Science & Climate:** Covers space exploration, environmental issues, and weather, including NASA news, astronomical discoveries, and climate change topics.\n",
       "\n",
       "**Announcements and Breaking News:**\n",
       "\n",
       "*   **Breaking News:** Hamas has handed over the final living hostages under the first phase of the Gaza ceasefire.\n",
       "*   **Hostage Situation:** Human remains returned by Hamas are confirmed to be those of Shiri Bibas.\n",
       "*   **US Politics:** Trump is set to address CPAC and is pursuing his agenda both domestically and internationally. There's news about Trump firing senior US military leadership in an unprecedented Pentagon purge.\n",
       "*   **International Conflict:** Russia is reportedly 'recycling' wounded troops to the frontlines in the Ukraine war.\n",
       "\n",
       "The website also includes sections for opinion pieces, investigations, and special features, as well as audio and video content, and options to subscribe to newsletters and listen to podcasts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://edition.cnn.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ca09e-4cca-49b9-ab57-f89acd3004ee",
   "metadata": {},
   "source": [
    "1. https://ai.google.dev/\n",
    "2. https://groq.com/\n",
    "3. https://build.nvidia.com/explore/discover\n",
    "4. https://openrouter.ai/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a1d855-b26e-4729-8bbc-034734aa58a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
